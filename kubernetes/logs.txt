* 
* ==> Audit <==
* |--------------|-----------------|----------|------|---------|-------------------------------|-------------------------------|
|   Command    |      Args       | Profile  | User | Version |          Start Time           |           End Time            |
|--------------|-----------------|----------|------|---------|-------------------------------|-------------------------------|
| start        |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 09:21:26 WIB | Sun, 27 Mar 2022 09:32:31 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 09:33:07 WIB | Sun, 27 Mar 2022 09:33:07 WIB |
| ssh          |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 09:34:40 WIB | Sun, 27 Mar 2022 09:35:02 WIB |
| start        |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 16:05:54 WIB | Sun, 27 Mar 2022 16:07:40 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 17:08:29 WIB | Sun, 27 Mar 2022 17:08:31 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 17:12:59 WIB | Sun, 27 Mar 2022 17:12:59 WIB |
| logs         | --file=logs.txt | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 17:26:59 WIB | Sun, 27 Mar 2022 17:27:10 WIB |
| ssh          |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 19:35:28 WIB | Sun, 27 Mar 2022 19:36:45 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 19:42:19 WIB | Sun, 27 Mar 2022 19:42:19 WIB |
| service      | k8s-web-hello   | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 19:44:12 WIB | Sun, 27 Mar 2022 19:44:12 WIB |
| service      | k8s-web-hello   | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 19:47:53 WIB | Sun, 27 Mar 2022 19:47:54 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Sun, 27 Mar 2022 19:49:11 WIB | Sun, 27 Mar 2022 19:49:11 WIB |
| start        |                 | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 14:33:29 WIB | Wed, 30 Mar 2022 07:36:47 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 07:46:11 WIB | Wed, 30 Mar 2022 07:46:12 WIB |
| service      | k8s-web-hello   | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 08:01:57 WIB | Wed, 30 Mar 2022 08:01:58 WIB |
| service      | k8s-web-hello   | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 08:03:05 WIB | Wed, 30 Mar 2022 08:03:06 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 08:03:31 WIB | Wed, 30 Mar 2022 08:03:31 WIB |
| service      | k8s-web-hello   | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 08:05:07 WIB | Wed, 30 Mar 2022 08:05:07 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 08:34:54 WIB | Wed, 30 Mar 2022 08:34:55 WIB |
| stop         |                 | minikube | aldi | v1.25.2 | Wed, 30 Mar 2022 08:44:41 WIB | Wed, 30 Mar 2022 08:45:06 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Fri, 01 Apr 2022 08:12:04 WIB | Fri, 01 Apr 2022 08:12:05 WIB |
| start        |                 | minikube | aldi | v1.25.2 | Fri, 01 Apr 2022 08:44:06 WIB | Fri, 01 Apr 2022 08:46:03 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Fri, 01 Apr 2022 08:56:36 WIB | Fri, 01 Apr 2022 08:56:37 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Fri, 01 Apr 2022 09:15:50 WIB | Fri, 01 Apr 2022 09:15:53 WIB |
| start        |                 | minikube | aldi | v1.25.2 | Wed, 19 Oct 2022 10:12:55 WIB | Wed, 19 Oct 2022 10:17:07 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Wed, 19 Oct 2022 10:59:17 WIB | Wed, 19 Oct 2022 10:59:18 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Wed, 26 Oct 2022 10:05:14 WIB | Wed, 26 Oct 2022 10:05:14 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Thu, 27 Oct 2022 07:09:40 WIB | Thu, 27 Oct 2022 07:09:43 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Wed, 02 Nov 2022 14:57:20 WIB | Wed, 02 Nov 2022 14:57:21 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Wed, 02 Nov 2022 15:13:22 WIB | Wed, 02 Nov 2022 15:13:23 WIB |
| start        |                 | minikube | aldi | v1.25.2 | Thu, 03 Nov 2022 08:17:11 WIB | Thu, 03 Nov 2022 08:19:39 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Thu, 03 Nov 2022 08:53:58 WIB | Thu, 03 Nov 2022 08:53:59 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Thu, 03 Nov 2022 09:18:45 WIB | Thu, 03 Nov 2022 09:18:46 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Thu, 03 Nov 2022 12:15:17 WIB | Thu, 03 Nov 2022 12:15:17 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Thu, 03 Nov 2022 15:28:40 WIB | Thu, 03 Nov 2022 15:28:40 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Thu, 03 Nov 2022 15:54:31 WIB | Thu, 03 Nov 2022 15:54:32 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Thu, 03 Nov 2022 20:21:37 WIB | Thu, 03 Nov 2022 20:21:38 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Sun, 06 Nov 2022 09:20:35 WIB | Sun, 06 Nov 2022 09:20:35 WIB |
| update-check |                 | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 18:38:13 WIB | Mon, 07 Nov 2022 18:38:13 WIB |
| start        |                 | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 18:43:52 WIB | Mon, 07 Nov 2022 18:47:35 WIB |
| service      | dtl-app         | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 19:47:36 WIB | Mon, 07 Nov 2022 19:47:48 WIB |
| service      | dtl-app         | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 19:48:03 WIB | Mon, 07 Nov 2022 19:48:04 WIB |
| service      | sdp-app         | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 19:58:44 WIB | Mon, 07 Nov 2022 19:58:48 WIB |
| service      | sdp-app         | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 20:01:22 WIB | Mon, 07 Nov 2022 20:01:27 WIB |
| service      | sdp-app         | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 20:02:39 WIB | Mon, 07 Nov 2022 20:02:41 WIB |
| service      | sdp-app         | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 20:15:15 WIB | Mon, 07 Nov 2022 20:15:21 WIB |
| ip           |                 | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 20:41:06 WIB | Mon, 07 Nov 2022 20:41:08 WIB |
| docker-env   |                 | minikube | aldi | v1.25.2 | Mon, 07 Nov 2022 20:55:48 WIB | Mon, 07 Nov 2022 20:55:56 WIB |
|--------------|-----------------|----------|------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/11/07 18:43:52
Running on machine: aldiubuntudesktop
Binary: Built with gc go1.17.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1107 18:43:52.649932    7537 out.go:297] Setting OutFile to fd 1 ...
I1107 18:43:52.650069    7537 out.go:349] isatty.IsTerminal(1) = true
I1107 18:43:52.650074    7537 out.go:310] Setting ErrFile to fd 2...
I1107 18:43:52.650080    7537 out.go:349] isatty.IsTerminal(2) = true
I1107 18:43:52.650294    7537 root.go:315] Updating PATH: /home/aldi/.minikube/bin
W1107 18:43:52.650426    7537 root.go:293] Error reading config file at /home/aldi/.minikube/config/config.json: open /home/aldi/.minikube/config/config.json: no such file or directory
I1107 18:43:52.650675    7537 out.go:304] Setting JSON to false
I1107 18:43:52.668472    7537 start.go:112] hostinfo: {"hostname":"aldiubuntudesktop","uptime":1376,"bootTime":1667820057,"procs":273,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.15.0-52-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"43c37787-36cb-4963-a023-59fe9bc6e3ed"}
I1107 18:43:52.668571    7537 start.go:122] virtualization: kvm host
I1107 18:43:52.708408    7537 out.go:176] üòÑ  minikube v1.25.2 on Ubuntu 20.04
I1107 18:43:52.708761    7537 notify.go:193] Checking for updates...
I1107 18:43:52.836167    7537 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I1107 18:43:52.859396    7537 driver.go:344] Setting default libvirt URI to qemu:///system
I1107 18:43:52.974295    7537 lock.go:35] WriteFile acquiring /home/aldi/.minikube/last_update_check: {Name:mkc388f0f96c3894a273cbc88baca237526b3cda Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1107 18:43:53.069022    7537 out.go:176] üéâ  minikube 1.28.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.28.0
I1107 18:43:53.151089    7537 out.go:176] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I1107 18:43:53.634756    7537 docker.go:132] docker version: linux-20.10.21
I1107 18:43:53.649793    7537 cli_runner.go:133] Run: docker system info --format "{{json .}}"
I1107 18:43:53.841499    7537 info.go:263] docker info: {ID:YSSJ:JTB7:PXD5:3IDZ:AIIU:6R54:RLIN:H2WF:2V26:623T:VY4E:U5XV Containers:18 ContainersRunning:0 ContainersPaused:0 ContainersStopped:18 Images:80 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2022-11-07 18:43:53.714176507 +0700 WIB LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-52-generic OperatingSystem:Ubuntu 20.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6166814720 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:aldiubuntudesktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f Expected:1c90a442489720eec95342e1789ee8a5e1b9536f} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1107 18:43:53.841610    7537 docker.go:237] overlay module found
I1107 18:43:53.899352    7537 out.go:176] ‚ú®  Using the docker driver based on existing profile
I1107 18:43:53.899409    7537 start.go:281] selected driver: docker
I1107 18:43:53.899419    7537 start.go:798] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aldi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false}
I1107 18:43:53.899842    7537 start.go:809] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1107 18:43:53.900245    7537 cli_runner.go:133] Run: docker system info --format "{{json .}}"
I1107 18:43:54.050471    7537 info.go:263] docker info: {ID:YSSJ:JTB7:PXD5:3IDZ:AIIU:6R54:RLIN:H2WF:2V26:623T:VY4E:U5XV Containers:18 ContainersRunning:0 ContainersPaused:0 ContainersStopped:18 Images:80 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2022-11-07 18:43:53.96367383 +0700 WIB LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-52-generic OperatingSystem:Ubuntu 20.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:6166814720 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:aldiubuntudesktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f Expected:1c90a442489720eec95342e1789ee8a5e1b9536f} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1107 18:43:54.068423    7537 cni.go:93] Creating CNI manager for ""
I1107 18:43:54.068455    7537 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1107 18:43:54.068472    7537 start_flags.go:302] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aldi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false}
I1107 18:43:54.111010    7537 out.go:176] üëç  Starting control plane node minikube in cluster minikube
I1107 18:43:54.111083    7537 cache.go:120] Beginning downloading kic base image for docker with docker
I1107 18:43:54.157641    7537 out.go:176] üöú  Pulling base image ...
I1107 18:43:54.157719    7537 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I1107 18:43:54.157806    7537 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon
I1107 18:43:54.157841    7537 preload.go:148] Found local preload: /home/aldi/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v17-v1.23.3-docker-overlay2-amd64.tar.lz4
I1107 18:43:54.157854    7537 cache.go:57] Caching tarball of preloaded images
I1107 18:43:54.158142    7537 preload.go:174] Found /home/aldi/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v17-v1.23.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1107 18:43:54.158184    7537 cache.go:60] Finished verifying existence of preloaded tar for  v1.23.3 on docker
I1107 18:43:54.158817    7537 profile.go:148] Saving config to /home/aldi/.minikube/profiles/minikube/config.json ...
I1107 18:43:54.215138    7537 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon, skipping pull
I1107 18:43:54.215170    7537 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 exists in daemon, skipping load
I1107 18:43:54.292241    7537 cache.go:208] Successfully downloaded all kic artifacts
I1107 18:43:54.292291    7537 start.go:313] acquiring machines lock for minikube: {Name:mk2ca22aa1e0c8877845fbb9546dbbb3d6697661 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1107 18:43:54.292480    7537 start.go:317] acquired machines lock for "minikube" in 162.404¬µs
I1107 18:43:54.292519    7537 start.go:93] Skipping create...Using existing machine configuration
I1107 18:43:54.292526    7537 fix.go:55] fixHost starting: 
I1107 18:43:54.293032    7537 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I1107 18:43:54.568024    7537 fix.go:108] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1107 18:43:54.568054    7537 fix.go:134] unexpected machine state, will restart: <nil>
I1107 18:43:54.679352    7537 out.go:176] üîÑ  Restarting existing docker container for "minikube" ...
I1107 18:43:54.679470    7537 cli_runner.go:133] Run: docker start minikube
I1107 18:44:00.883082    7537 cli_runner.go:186] Completed: docker start minikube: (6.203573127s)
I1107 18:44:00.883268    7537 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I1107 18:44:00.915384    7537 kic.go:420] container "minikube" state is running.
I1107 18:44:00.916024    7537 cli_runner.go:133] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1107 18:44:00.980676    7537 profile.go:148] Saving config to /home/aldi/.minikube/profiles/minikube/config.json ...
I1107 18:44:00.981060    7537 machine.go:88] provisioning docker machine ...
I1107 18:44:00.981083    7537 ubuntu.go:169] provisioning hostname "minikube"
I1107 18:44:00.981204    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:01.019173    7537 main.go:130] libmachine: Using SSH client type: native
I1107 18:44:01.030597    7537 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1107 18:44:01.030622    7537 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1107 18:44:01.031386    7537 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:40606->127.0.0.1:49157: read: connection reset by peer
I1107 18:44:04.032890    7537 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:40608->127.0.0.1:49157: read: connection reset by peer
I1107 18:44:08.565376    7537 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I1107 18:44:08.565474    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:08.607140    7537 main.go:130] libmachine: Using SSH client type: native
I1107 18:44:08.607550    7537 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1107 18:44:08.607586    7537 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1107 18:44:08.797131    7537 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I1107 18:44:08.797163    7537 ubuntu.go:175] set auth options {CertDir:/home/aldi/.minikube CaCertPath:/home/aldi/.minikube/certs/ca.pem CaPrivateKeyPath:/home/aldi/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/aldi/.minikube/machines/server.pem ServerKeyPath:/home/aldi/.minikube/machines/server-key.pem ClientKeyPath:/home/aldi/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/aldi/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/aldi/.minikube}
I1107 18:44:08.797219    7537 ubuntu.go:177] setting up certificates
I1107 18:44:08.797232    7537 provision.go:83] configureAuth start
I1107 18:44:08.797375    7537 cli_runner.go:133] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1107 18:44:08.857648    7537 provision.go:138] copyHostCerts
I1107 18:44:09.009586    7537 exec_runner.go:144] found /home/aldi/.minikube/key.pem, removing ...
I1107 18:44:09.009599    7537 exec_runner.go:207] rm: /home/aldi/.minikube/key.pem
I1107 18:44:09.040352    7537 exec_runner.go:151] cp: /home/aldi/.minikube/certs/key.pem --> /home/aldi/.minikube/key.pem (1679 bytes)
I1107 18:44:09.145878    7537 exec_runner.go:144] found /home/aldi/.minikube/ca.pem, removing ...
I1107 18:44:09.145899    7537 exec_runner.go:207] rm: /home/aldi/.minikube/ca.pem
I1107 18:44:09.145986    7537 exec_runner.go:151] cp: /home/aldi/.minikube/certs/ca.pem --> /home/aldi/.minikube/ca.pem (1070 bytes)
I1107 18:44:09.156704    7537 exec_runner.go:144] found /home/aldi/.minikube/cert.pem, removing ...
I1107 18:44:09.156731    7537 exec_runner.go:207] rm: /home/aldi/.minikube/cert.pem
I1107 18:44:09.156788    7537 exec_runner.go:151] cp: /home/aldi/.minikube/certs/cert.pem --> /home/aldi/.minikube/cert.pem (1115 bytes)
I1107 18:44:09.157470    7537 provision.go:112] generating server cert: /home/aldi/.minikube/machines/server.pem ca-key=/home/aldi/.minikube/certs/ca.pem private-key=/home/aldi/.minikube/certs/ca-key.pem org=aldi.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1107 18:44:09.423462    7537 provision.go:172] copyRemoteCerts
I1107 18:44:09.457010    7537 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1107 18:44:09.457083    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:09.797626    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:44:10.229918    7537 ssh_runner.go:362] scp /home/aldi/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I1107 18:44:10.374006    7537 ssh_runner.go:362] scp /home/aldi/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1107 18:44:10.403419    7537 ssh_runner.go:362] scp /home/aldi/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1107 18:44:10.432373    7537 provision.go:86] duration metric: configureAuth took 1.635126098s
I1107 18:44:10.432399    7537 ubuntu.go:193] setting minikube options for container-runtime
I1107 18:44:10.432603    7537 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I1107 18:44:10.432646    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:10.492898    7537 main.go:130] libmachine: Using SSH client type: native
I1107 18:44:10.493044    7537 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1107 18:44:10.493053    7537 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1107 18:44:10.672476    7537 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I1107 18:44:10.672500    7537 ubuntu.go:71] root file system type: overlay
I1107 18:44:10.672851    7537 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1107 18:44:10.672957    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:10.708913    7537 main.go:130] libmachine: Using SSH client type: native
I1107 18:44:10.709151    7537 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1107 18:44:10.709307    7537 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1107 18:44:10.875944    7537 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1107 18:44:10.876065    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:10.910002    7537 main.go:130] libmachine: Using SSH client type: native
I1107 18:44:10.910257    7537 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x7a12c0] 0x7a43a0 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1107 18:44:10.910289    7537 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1107 18:44:11.066126    7537 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I1107 18:44:11.066150    7537 machine.go:91] provisioned docker machine in 10.08507786s
I1107 18:44:11.066166    7537 start.go:267] post-start starting for "minikube" (driver="docker")
I1107 18:44:11.066176    7537 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1107 18:44:11.066261    7537 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1107 18:44:11.066323    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:11.093399    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:44:11.367887    7537 ssh_runner.go:195] Run: cat /etc/os-release
I1107 18:44:11.390740    7537 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1107 18:44:11.390770    7537 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1107 18:44:11.390790    7537 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1107 18:44:11.390799    7537 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I1107 18:44:11.390814    7537 filesync.go:126] Scanning /home/aldi/.minikube/addons for local assets ...
I1107 18:44:11.416208    7537 filesync.go:126] Scanning /home/aldi/.minikube/files for local assets ...
I1107 18:44:11.438461    7537 start.go:270] post-start completed in 372.281386ms
I1107 18:44:11.438558    7537 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1107 18:44:11.438652    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:11.509931    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:44:11.630447    7537 fix.go:57] fixHost completed within 17.337913823s
I1107 18:44:11.630468    7537 start.go:80] releasing machines lock for "minikube", held for 17.337974326s
I1107 18:44:11.630589    7537 cli_runner.go:133] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1107 18:44:11.674990    7537 ssh_runner.go:195] Run: systemctl --version
I1107 18:44:11.675010    7537 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1107 18:44:11.675031    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:11.675050    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:44:11.704519    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:44:11.920548    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:44:14.234092    7537 ssh_runner.go:235] Completed: systemctl --version: (2.559064592s)
I1107 18:44:14.234201    7537 ssh_runner.go:235] Completed: curl -sS -m 2 https://k8s.gcr.io/: (2.559166362s)
I1107 18:44:14.234220    7537 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1107 18:44:14.292341    7537 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1107 18:44:14.305876    7537 cruntime.go:272] skipping containerd shutdown because we are bound to it
I1107 18:44:14.305968    7537 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1107 18:44:14.386816    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I1107 18:44:14.409208    7537 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1107 18:44:14.826374    7537 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1107 18:44:14.946611    7537 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1107 18:44:14.964782    7537 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1107 18:44:15.084536    7537 ssh_runner.go:195] Run: sudo systemctl start docker
I1107 18:44:31.004600    7537 ssh_runner.go:235] Completed: sudo systemctl start docker: (15.92003936s)
I1107 18:44:31.004658    7537 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1107 18:44:33.945853    7537 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (2.941146467s)
I1107 18:44:33.945926    7537 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1107 18:44:34.173488    7537 out.go:203] üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
I1107 18:44:34.173636    7537 cli_runner.go:133] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1107 18:44:34.443720    7537 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1107 18:44:34.465238    7537 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1107 18:44:34.676167    7537 out.go:176]     ‚ñ™ kubelet.housekeeping-interval=5m
I1107 18:44:34.704305    7537 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I1107 18:44:34.704478    7537 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1107 18:44:34.777626    7537 docker.go:606] Got preloaded images: -- stdout --
busybox:latest
nginx:latest
zull123/kubernetes-node-to-nginx:latest
zull123/kubernetes-node:latest
nginx:<none>
zull123/kubernetes-node:<none>
mongo:5.0
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
nanajanashia/k8s-demo-app:v1.0
k8s.gcr.io/pause:3.6
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1107 18:44:34.791148    7537 docker.go:537] Images already preloaded, skipping extraction
I1107 18:44:34.791306    7537 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1107 18:44:34.841391    7537 docker.go:606] Got preloaded images: -- stdout --
busybox:latest
nginx:latest
zull123/kubernetes-node-to-nginx:latest
zull123/kubernetes-node:latest
nginx:<none>
zull123/kubernetes-node:<none>
mongo:5.0
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
nanajanashia/k8s-demo-app:v1.0
k8s.gcr.io/pause:3.6
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1107 18:44:34.841407    7537 cache_images.go:84] Images are preloaded, skipping loading
I1107 18:44:34.841488    7537 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1107 18:44:38.897962    7537 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (4.05645706s)
I1107 18:44:38.898002    7537 cni.go:93] Creating CNI manager for ""
I1107 18:44:38.898011    7537 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1107 18:44:38.898017    7537 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1107 18:44:38.916290    7537 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.23.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1107 18:44:38.916551    7537 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.23.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1107 18:44:38.916692    7537 kubeadm.go:936] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.23.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --housekeeping-interval=5m --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1107 18:44:38.916768    7537 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.23.3
I1107 18:44:39.020048    7537 binaries.go:44] Found k8s binaries, skipping transfer
I1107 18:44:39.020136    7537 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1107 18:44:39.032851    7537 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (361 bytes)
I1107 18:44:39.095709    7537 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1107 18:44:39.117152    7537 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I1107 18:44:39.161045    7537 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1107 18:44:39.164258    7537 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1107 18:44:39.176083    7537 certs.go:54] Setting up /home/aldi/.minikube/profiles/minikube for IP: 192.168.49.2
I1107 18:44:39.176184    7537 certs.go:182] skipping minikubeCA CA generation: /home/aldi/.minikube/ca.key
I1107 18:44:39.226129    7537 certs.go:182] skipping proxyClientCA CA generation: /home/aldi/.minikube/proxy-client-ca.key
I1107 18:44:39.226298    7537 certs.go:298] skipping minikube-user signed cert generation: /home/aldi/.minikube/profiles/minikube/client.key
I1107 18:44:39.226580    7537 certs.go:298] skipping minikube signed cert generation: /home/aldi/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1107 18:44:39.226837    7537 certs.go:298] skipping aggregator signed cert generation: /home/aldi/.minikube/profiles/minikube/proxy-client.key
I1107 18:44:39.227060    7537 certs.go:388] found cert: /home/aldi/.minikube/certs/home/aldi/.minikube/certs/ca-key.pem (1679 bytes)
I1107 18:44:39.227108    7537 certs.go:388] found cert: /home/aldi/.minikube/certs/home/aldi/.minikube/certs/ca.pem (1070 bytes)
I1107 18:44:39.227145    7537 certs.go:388] found cert: /home/aldi/.minikube/certs/home/aldi/.minikube/certs/cert.pem (1115 bytes)
I1107 18:44:39.227180    7537 certs.go:388] found cert: /home/aldi/.minikube/certs/home/aldi/.minikube/certs/key.pem (1679 bytes)
I1107 18:44:39.272241    7537 ssh_runner.go:362] scp /home/aldi/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1107 18:44:39.330034    7537 ssh_runner.go:362] scp /home/aldi/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1107 18:44:39.361856    7537 ssh_runner.go:362] scp /home/aldi/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1107 18:44:39.392451    7537 ssh_runner.go:362] scp /home/aldi/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1107 18:44:39.424489    7537 ssh_runner.go:362] scp /home/aldi/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1107 18:44:39.448532    7537 ssh_runner.go:362] scp /home/aldi/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1107 18:44:39.478942    7537 ssh_runner.go:362] scp /home/aldi/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1107 18:44:39.507110    7537 ssh_runner.go:362] scp /home/aldi/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1107 18:44:39.536026    7537 ssh_runner.go:362] scp /home/aldi/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1107 18:44:39.590681    7537 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1107 18:44:39.666756    7537 ssh_runner.go:195] Run: openssl version
I1107 18:44:39.748458    7537 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1107 18:44:39.779695    7537 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1107 18:44:39.785389    7537 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Mar 27  2022 /usr/share/ca-certificates/minikubeCA.pem
I1107 18:44:39.785466    7537 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1107 18:44:39.819994    7537 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1107 18:44:39.831950    7537 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/aldi:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false}
I1107 18:44:39.832086    7537 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1107 18:44:39.868740    7537 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1107 18:44:39.892490    7537 kubeadm.go:402] found existing configuration files, will attempt cluster restart
I1107 18:44:39.892511    7537 kubeadm.go:601] restartCluster start
I1107 18:44:39.892580    7537 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1107 18:44:39.905181    7537 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1107 18:44:39.914898    7537 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I1107 18:44:40.037919    7537 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1107 18:44:40.090995    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:40.091067    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:40.184071    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:40.384357    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:40.384463    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:40.409817    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:40.585149    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:40.585258    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:40.608162    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:40.784400    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:40.784505    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:40.810005    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:40.984178    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:40.984275    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:41.007584    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:41.184755    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:41.184827    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:41.210007    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:41.384200    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:41.384279    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:41.411837    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:41.585005    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:41.585096    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:41.607828    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:41.785045    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:41.785158    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:41.814727    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:41.984971    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:41.985074    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:42.011907    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:42.185114    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:42.185211    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:42.208568    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:42.384788    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:42.384881    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:42.411968    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:42.584503    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:42.584579    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:42.614399    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:42.784621    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:42.784708    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:42.806911    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:42.985172    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:42.985268    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:43.013297    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:43.184352    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:43.184431    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:43.209933    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:43.209945    7537 api_server.go:165] Checking apiserver status ...
I1107 18:44:43.209989    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1107 18:44:43.226708    7537 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1107 18:44:43.226731    7537 kubeadm.go:576] needs reconfigure: apiserver error: timed out waiting for the condition
I1107 18:44:43.226741    7537 kubeadm.go:1067] stopping kube-system containers ...
I1107 18:44:43.226822    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1107 18:44:43.353133    7537 docker.go:438] Stopping containers: [86972bf0f152 e9b5d500ebe0 e9f3806e9153 d5961837e171 6c3feed8aea5 e9b6ef5eccf7 604bf93ed185 7cbb8a163e77 723efb2ab753 0507240798b3 6143cc5693fa 0b019d8f66f9 99440876db1b ab091607a241 bda45d603ece b25e789c7449 2388291fdd5f e0b9f8ff8690 15df375a6b74 535cf064c8b2 136412b57ab1 db83bda2554a 9280a7fd8675 9c4ca5cc10c1 1ced0c5b9a91 d7ca808542eb de36aa80c653]
I1107 18:44:43.353234    7537 ssh_runner.go:195] Run: docker stop 86972bf0f152 e9b5d500ebe0 e9f3806e9153 d5961837e171 6c3feed8aea5 e9b6ef5eccf7 604bf93ed185 7cbb8a163e77 723efb2ab753 0507240798b3 6143cc5693fa 0b019d8f66f9 99440876db1b ab091607a241 bda45d603ece b25e789c7449 2388291fdd5f e0b9f8ff8690 15df375a6b74 535cf064c8b2 136412b57ab1 db83bda2554a 9280a7fd8675 9c4ca5cc10c1 1ced0c5b9a91 d7ca808542eb de36aa80c653
I1107 18:44:43.431567    7537 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1107 18:44:43.466283    7537 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1107 18:44:43.477537    7537 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Mar 27  2022 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Nov  3 01:18 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar 27  2022 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Nov  3 01:18 /etc/kubernetes/scheduler.conf

I1107 18:44:43.477581    7537 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1107 18:44:43.485528    7537 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1107 18:44:43.512171    7537 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1107 18:44:43.539840    7537 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1107 18:44:43.539910    7537 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1107 18:44:43.564711    7537 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1107 18:44:43.578796    7537 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1107 18:44:43.578881    7537 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1107 18:44:43.585445    7537 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1107 18:44:43.597437    7537 kubeadm.go:678] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1107 18:44:43.666107    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1107 18:44:45.407969    7537 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (1.741827218s)
I1107 18:44:45.408006    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1107 18:44:46.429516    7537 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.021491786s)
I1107 18:44:46.429536    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1107 18:44:46.799053    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1107 18:44:46.869635    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1107 18:44:46.925865    7537 api_server.go:51] waiting for apiserver process to appear ...
I1107 18:44:46.925935    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:47.483335    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:47.983051    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:48.482763    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:48.983130    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:49.483546    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:49.983174    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:50.482880    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:50.982808    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:51.483169    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:51.983455    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:52.483723    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:52.983398    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:53.482776    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:53.983304    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:54.482976    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:54.983362    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:55.482773    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:55.982854    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:56.482807    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:56.982876    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:57.483601    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:57.983557    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:58.483119    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:58.982746    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:59.483088    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:44:59.983555    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:00.483031    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:00.983611    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:01.482955    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:01.983249    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:02.482817    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:02.983361    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:03.483525    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:03.982900    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:04.482790    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:04.982773    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:05.483192    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:05.982987    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:06.483486    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:06.982787    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:07.482713    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:07.983307    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:08.483701    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:08.983358    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:09.483140    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:09.983160    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:10.482760    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:10.982757    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:11.483003    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:11.983049    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:12.483686    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:12.983390    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:13.483329    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:13.983212    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:14.482711    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:14.983662    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:15.483652    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:45:15.514925    7537 api_server.go:71] duration metric: took 28.589062926s to wait for apiserver process to appear ...
I1107 18:45:15.514939    7537 api_server.go:87] waiting for apiserver healthz status ...
I1107 18:45:15.514949    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:15.515282    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:16.015909    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:16.016442    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:16.516113    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:16.516577    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:17.016165    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:17.016640    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:17.516357    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:17.929099    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:18.015559    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:18.016214    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:18.516393    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:18.516943    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:19.016367    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:19.016848    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:19.516367    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:19.516830    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:20.015441    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:20.016011    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:20.516408    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:20.516955    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:21.016420    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:21.017020    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:21.516398    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:21.516943    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:22.016407    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:22.016926    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:22.515797    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:22.638454    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:23.015991    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:23.016422    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:23.516147    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:23.516761    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:24.016350    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:24.016825    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:24.516364    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:24.516837    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:25.016398    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:25.016916    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:25.516401    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:25.516948    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:26.016334    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:26.016837    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:26.516407    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:26.516985    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:27.015453    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:27.015772    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:27.515461    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:27.634034    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:28.016389    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:28.017153    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:28.516393    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:28.516927    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:29.016470    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:29.017037    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:29.516409    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:29.516906    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:30.015445    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:30.015978    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:30.515486    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:30.515977    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:31.016397    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:31.016950    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:31.516379    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:31.516738    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:32.016340    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:32.016827    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:32.515861    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:32.659052    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:33.019318    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:33.019856    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:33.516371    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:33.516902    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:34.016366    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:34.016915    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:34.516377    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:34.516875    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:35.016381    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:35.016959    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:35.516363    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:35.516827    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:36.016425    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:36.016883    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:36.516397    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:36.516957    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:37.015489    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:37.016216    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:37.515979    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:37.726426    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:38.015994    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:38.016593    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:38.516199    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:38.516698    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:39.015907    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:39.016499    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:39.516095    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:39.516599    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:40.016314    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:40.016903    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:40.516395    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:40.516929    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:41.016389    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:41.016945    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:41.515458    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:41.515893    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:42.016386    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:42.016833    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:42.516303    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:42.667012    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:43.016324    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:43.016701    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:43.516313    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:43.516848    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:44.016349    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:44.017074    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1107 18:45:44.515511    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:49.515762    7537 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1107 18:45:50.015452    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:50.184215    7537 api_server.go:266] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1107 18:45:50.184232    7537 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1107 18:45:50.516350    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:50.602550    7537 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W1107 18:45:50.602592    7537 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I1107 18:45:51.015700    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:51.032148    7537 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W1107 18:45:51.032174    7537 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I1107 18:45:51.516358    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:45:51.523075    7537 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I1107 18:45:51.791280    7537 api_server.go:140] control plane version: v1.23.3
I1107 18:45:51.791305    7537 api_server.go:130] duration metric: took 36.27635881s to wait for apiserver health ...
I1107 18:45:51.791320    7537 cni.go:93] Creating CNI manager for ""
I1107 18:45:51.791332    7537 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I1107 18:45:51.880883    7537 system_pods.go:43] waiting for kube-system pods to appear ...
I1107 18:45:52.573697    7537 system_pods.go:59] 7 kube-system pods found
I1107 18:45:52.610271    7537 system_pods.go:61] "coredns-64897985d-frt6z" [e4a702f5-3213-4cb0-b4bd-eceb45dc04af] Running
I1107 18:45:52.610284    7537 system_pods.go:61] "etcd-minikube" [24f782c8-2112-4472-b866-b6caf13582a7] Running
I1107 18:45:52.610292    7537 system_pods.go:61] "kube-apiserver-minikube" [b5c530d8-fad7-447e-8cd2-87fbb6b772b6] Running
I1107 18:45:52.610300    7537 system_pods.go:61] "kube-controller-manager-minikube" [d6d58232-b03d-40cd-a361-c97cb49297a1] Running
I1107 18:45:52.610309    7537 system_pods.go:61] "kube-proxy-ddgh5" [922488aa-2e76-4c96-8e3f-9a69fab97b5a] Running
I1107 18:45:52.610317    7537 system_pods.go:61] "kube-scheduler-minikube" [3d470ed2-a74d-4019-86e8-c5c52e3dec54] Running
I1107 18:45:52.610324    7537 system_pods.go:61] "storage-provisioner" [fad20af1-8fe0-480e-9995-856be38ecdeb] Running
I1107 18:45:52.610330    7537 system_pods.go:74] duration metric: took 729.433388ms to wait for pod list to return data ...
I1107 18:45:52.610359    7537 node_conditions.go:102] verifying NodePressure condition ...
I1107 18:45:52.716647    7537 node_conditions.go:122] node storage ephemeral capacity is 200475920Ki
I1107 18:45:52.716679    7537 node_conditions.go:123] node cpu capacity is 4
I1107 18:45:52.716695    7537 node_conditions.go:105] duration metric: took 106.329144ms to run NodePressure ...
I1107 18:45:52.716724    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1107 18:45:55.327145    7537 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.610389306s)
I1107 18:45:55.327174    7537 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1107 18:45:55.535337    7537 ops.go:34] apiserver oom_adj: -16
I1107 18:45:55.535362    7537 kubeadm.go:605] restartCluster took 1m15.642841017s
I1107 18:45:55.535374    7537 kubeadm.go:393] StartCluster complete in 1m15.703431046s
I1107 18:45:55.535397    7537 settings.go:142] acquiring lock: {Name:mk63f00e2d7b73f880f3d847b3852a40734c5952 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1107 18:45:55.535597    7537 settings.go:150] Updating kubeconfig:  /home/aldi/.kube/config
I1107 18:45:55.536707    7537 lock.go:35] WriteFile acquiring /home/aldi/.kube/config: {Name:mkdfa96bf26335f0667f6932500babca237a4bdb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1107 18:45:55.573607    7537 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1107 18:45:55.573675    7537 start.go:208] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1107 18:45:55.573741    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1107 18:45:55.646505    7537 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I1107 18:45:55.728450    7537 out.go:176] üîé  Verifying Kubernetes components...
I1107 18:45:55.689929    7537 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1107 18:45:55.728629    7537 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1107 18:45:55.728657    7537 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I1107 18:45:55.728658    7537 addons.go:65] Setting dashboard=true in profile "minikube"
W1107 18:45:55.728667    7537 addons.go:165] addon storage-provisioner should already be in state true
I1107 18:45:55.728688    7537 addons.go:153] Setting addon dashboard=true in "minikube"
W1107 18:45:55.728702    7537 addons.go:165] addon dashboard should already be in state true
I1107 18:45:55.728815    7537 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1107 18:45:55.728839    7537 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1107 18:45:55.729355    7537 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1107 18:45:55.776835    7537 host.go:66] Checking if "minikube" exists ...
I1107 18:45:55.776839    7537 host.go:66] Checking if "minikube" exists ...
I1107 18:45:55.861435    7537 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I1107 18:45:55.861435    7537 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I1107 18:45:56.007053    7537 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I1107 18:45:56.883716    7537 out.go:176]     ‚ñ™ Using image kubernetesui/dashboard:v2.3.1
I1107 18:45:56.975254    7537 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1107 18:45:57.195661    7537 addons.go:165] addon default-storageclass should already be in state true
I1107 18:45:57.195664    7537 out.go:176]     ‚ñ™ Using image kubernetesui/metrics-scraper:v1.0.7
I1107 18:45:57.128533    7537 out.go:176]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1107 18:45:57.195728    7537 host.go:66] Checking if "minikube" exists ...
I1107 18:45:57.195905    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1107 18:45:57.195927    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1107 18:45:57.195954    7537 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1107 18:45:57.195970    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1107 18:45:57.196048    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:45:57.196050    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:45:57.196924    7537 cli_runner.go:133] Run: docker container inspect minikube --format={{.State.Status}}
I1107 18:45:57.241832    7537 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I1107 18:45:57.241845    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1107 18:45:57.241900    7537 cli_runner.go:133] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1107 18:45:57.927803    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:45:57.928910    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:45:57.930919    7537 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/aldi/.minikube/machines/minikube/id_rsa Username:docker}
I1107 18:45:59.844069    7537 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1107 18:45:59.844424    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1107 18:45:59.844443    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1107 18:45:59.844556    7537 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1107 18:45:59.875618    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1107 18:45:59.875640    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1107 18:46:00.006968    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1107 18:46:00.006993    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1107 18:46:00.044153    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1107 18:46:00.044165    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4278 bytes)
I1107 18:46:00.064189    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-role.yaml
I1107 18:46:00.064203    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1107 18:46:00.084723    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1107 18:46:00.084739    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1107 18:46:00.120638    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1107 18:46:00.120659    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1107 18:46:00.146153    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1107 18:46:00.146177    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1107 18:46:00.165321    7537 addons.go:348] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1107 18:46:00.165349    7537 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1107 18:46:00.188242    7537 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1107 18:47:06.391980    7537 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1m10.662591307s)
I1107 18:47:06.392027    7537 api_server.go:51] waiting for apiserver process to appear ...
I1107 18:47:06.392200    7537 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1m10.663794322s)
I1107 18:47:06.392285    7537 start.go:757] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1107 18:47:06.642911    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1107 18:47:08.950271    7537 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1m9.106172515s)
I1107 18:47:08.950283    7537 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1m9.105715956s)
I1107 18:47:11.073943    7537 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1m10.885666959s)
I1107 18:47:11.967468    7537 out.go:176] üåü  Enabled addons: storage-provisioner, default-storageclass, dashboard
I1107 18:47:11.967542    7537 addons.go:417] enableAddons completed in 1m16.393793419s
I1107 18:47:16.808649    7537 ssh_runner.go:235] Completed: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}: (10.16569497s)
I1107 18:47:16.808680    7537 logs.go:274] 2 containers: [49b4ef3c7b74 7cbb8a163e77]
I1107 18:47:16.808773    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1107 18:47:16.860642    7537 logs.go:274] 2 containers: [da4cf9e61ce8 6143cc5693fa]
I1107 18:47:16.860708    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1107 18:47:16.905973    7537 logs.go:274] 2 containers: [b6534c6ee5b0 e9b5d500ebe0]
I1107 18:47:16.906031    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1107 18:47:16.948225    7537 logs.go:274] 2 containers: [1ed89d5b1d77 723efb2ab753]
I1107 18:47:16.948324    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1107 18:47:16.987727    7537 logs.go:274] 1 containers: [e299c4606fa5]
I1107 18:47:16.987819    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1107 18:47:17.041006    7537 logs.go:274] 2 containers: [66987a5f16e5 95b2c70750d1]
I1107 18:47:17.041087    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1107 18:47:17.074306    7537 logs.go:274] 2 containers: [565aa3f294e0 7eba8c15d7c6]
I1107 18:47:17.074381    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1107 18:47:17.122315    7537 logs.go:274] 2 containers: [5e666a149161 0507240798b3]
I1107 18:47:17.122343    7537 logs.go:123] Gathering logs for etcd [6143cc5693fa] ...
I1107 18:47:17.122353    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6143cc5693fa"
I1107 18:47:17.719139    7537 logs.go:123] Gathering logs for coredns [b6534c6ee5b0] ...
I1107 18:47:17.719165    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b6534c6ee5b0"
I1107 18:47:17.761980    7537 logs.go:123] Gathering logs for kubernetes-dashboard [66987a5f16e5] ...
I1107 18:47:17.762001    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 66987a5f16e5"
I1107 18:47:17.912980    7537 logs.go:123] Gathering logs for storage-provisioner [7eba8c15d7c6] ...
I1107 18:47:17.913004    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7eba8c15d7c6"
I1107 18:47:17.952565    7537 logs.go:123] Gathering logs for Docker ...
I1107 18:47:17.952586    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1107 18:47:18.345832    7537 logs.go:123] Gathering logs for kube-apiserver [7cbb8a163e77] ...
I1107 18:47:18.345854    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7cbb8a163e77"
I1107 18:47:18.782014    7537 logs.go:123] Gathering logs for kubernetes-dashboard [95b2c70750d1] ...
I1107 18:47:18.782030    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95b2c70750d1"
I1107 18:47:18.847143    7537 logs.go:123] Gathering logs for kube-controller-manager [5e666a149161] ...
I1107 18:47:18.847166    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5e666a149161"
I1107 18:47:19.096391    7537 logs.go:123] Gathering logs for kube-controller-manager [0507240798b3] ...
I1107 18:47:19.096410    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0507240798b3"
I1107 18:47:19.248537    7537 logs.go:123] Gathering logs for kube-proxy [e299c4606fa5] ...
I1107 18:47:19.248558    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e299c4606fa5"
I1107 18:47:19.299811    7537 logs.go:123] Gathering logs for kube-apiserver [49b4ef3c7b74] ...
I1107 18:47:19.299834    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 49b4ef3c7b74"
I1107 18:47:19.617481    7537 logs.go:123] Gathering logs for etcd [da4cf9e61ce8] ...
I1107 18:47:19.617503    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 da4cf9e61ce8"
I1107 18:47:19.798525    7537 logs.go:123] Gathering logs for kube-scheduler [1ed89d5b1d77] ...
I1107 18:47:19.798542    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1ed89d5b1d77"
I1107 18:47:20.099685    7537 logs.go:123] Gathering logs for kube-scheduler [723efb2ab753] ...
I1107 18:47:20.099703    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 723efb2ab753"
I1107 18:47:20.257512    7537 logs.go:123] Gathering logs for storage-provisioner [565aa3f294e0] ...
I1107 18:47:20.257530    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 565aa3f294e0"
I1107 18:47:20.298785    7537 logs.go:123] Gathering logs for container status ...
I1107 18:47:20.298802    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1107 18:47:21.624991    7537 ssh_runner.go:235] Completed: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a": (1.32617166s)
I1107 18:47:21.627157    7537 logs.go:123] Gathering logs for kubelet ...
I1107 18:47:21.627176    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1107 18:47:21.719943    7537 logs.go:123] Gathering logs for describe nodes ...
I1107 18:47:21.719967    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1107 18:47:22.752918    7537 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (1.032927459s)
I1107 18:47:22.757776    7537 logs.go:123] Gathering logs for coredns [e9b5d500ebe0] ...
I1107 18:47:22.757796    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e9b5d500ebe0"
I1107 18:47:22.814334    7537 logs.go:123] Gathering logs for dmesg ...
I1107 18:47:22.814353    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1107 18:47:25.689372    7537 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1107 18:47:25.799093    7537 api_server.go:71] duration metric: took 1m30.22538238s to wait for apiserver process to appear ...
I1107 18:47:25.799108    7537 api_server.go:87] waiting for apiserver healthz status ...
I1107 18:47:25.799160    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1107 18:47:25.843908    7537 logs.go:274] 2 containers: [49b4ef3c7b74 7cbb8a163e77]
I1107 18:47:25.843970    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1107 18:47:25.891906    7537 logs.go:274] 2 containers: [da4cf9e61ce8 6143cc5693fa]
I1107 18:47:25.891964    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1107 18:47:25.935566    7537 logs.go:274] 2 containers: [b6534c6ee5b0 e9b5d500ebe0]
I1107 18:47:25.935642    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1107 18:47:26.094926    7537 logs.go:274] 2 containers: [1ed89d5b1d77 723efb2ab753]
I1107 18:47:26.095009    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1107 18:47:26.141657    7537 logs.go:274] 1 containers: [e299c4606fa5]
I1107 18:47:26.141713    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1107 18:47:26.186942    7537 logs.go:274] 2 containers: [66987a5f16e5 95b2c70750d1]
I1107 18:47:26.187003    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1107 18:47:26.225573    7537 logs.go:274] 2 containers: [565aa3f294e0 7eba8c15d7c6]
I1107 18:47:26.225690    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1107 18:47:26.271565    7537 logs.go:274] 2 containers: [5e666a149161 0507240798b3]
I1107 18:47:26.271587    7537 logs.go:123] Gathering logs for kube-apiserver [49b4ef3c7b74] ...
I1107 18:47:26.271595    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 49b4ef3c7b74"
I1107 18:47:26.358700    7537 logs.go:123] Gathering logs for etcd [6143cc5693fa] ...
I1107 18:47:26.358718    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6143cc5693fa"
I1107 18:47:26.513545    7537 logs.go:123] Gathering logs for coredns [b6534c6ee5b0] ...
I1107 18:47:26.513563    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b6534c6ee5b0"
I1107 18:47:26.556002    7537 logs.go:123] Gathering logs for kube-proxy [e299c4606fa5] ...
I1107 18:47:26.556028    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e299c4606fa5"
I1107 18:47:26.595423    7537 logs.go:123] Gathering logs for kubernetes-dashboard [66987a5f16e5] ...
I1107 18:47:26.595440    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 66987a5f16e5"
I1107 18:47:26.632082    7537 logs.go:123] Gathering logs for storage-provisioner [565aa3f294e0] ...
I1107 18:47:26.632098    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 565aa3f294e0"
I1107 18:47:26.676404    7537 logs.go:123] Gathering logs for storage-provisioner [7eba8c15d7c6] ...
I1107 18:47:26.676431    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7eba8c15d7c6"
I1107 18:47:26.715855    7537 logs.go:123] Gathering logs for describe nodes ...
I1107 18:47:26.715871    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1107 18:47:26.822182    7537 logs.go:123] Gathering logs for coredns [e9b5d500ebe0] ...
I1107 18:47:26.822203    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e9b5d500ebe0"
I1107 18:47:26.865375    7537 logs.go:123] Gathering logs for kube-scheduler [723efb2ab753] ...
I1107 18:47:26.865393    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 723efb2ab753"
I1107 18:47:26.933641    7537 logs.go:123] Gathering logs for container status ...
I1107 18:47:26.933659    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1107 18:47:27.219730    7537 logs.go:123] Gathering logs for kubelet ...
I1107 18:47:27.219754    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1107 18:47:27.305183    7537 logs.go:123] Gathering logs for kube-controller-manager [5e666a149161] ...
I1107 18:47:27.305206    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5e666a149161"
I1107 18:47:27.371117    7537 logs.go:123] Gathering logs for kube-controller-manager [0507240798b3] ...
I1107 18:47:27.371142    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0507240798b3"
I1107 18:47:27.439367    7537 logs.go:123] Gathering logs for etcd [da4cf9e61ce8] ...
I1107 18:47:27.439385    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 da4cf9e61ce8"
I1107 18:47:27.581108    7537 logs.go:123] Gathering logs for kube-apiserver [7cbb8a163e77] ...
I1107 18:47:27.581125    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7cbb8a163e77"
I1107 18:47:27.661818    7537 logs.go:123] Gathering logs for kube-scheduler [1ed89d5b1d77] ...
I1107 18:47:27.661837    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1ed89d5b1d77"
I1107 18:47:27.765200    7537 logs.go:123] Gathering logs for kubernetes-dashboard [95b2c70750d1] ...
I1107 18:47:27.765219    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95b2c70750d1"
I1107 18:47:27.804437    7537 logs.go:123] Gathering logs for Docker ...
I1107 18:47:27.804452    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1107 18:47:27.828061    7537 logs.go:123] Gathering logs for dmesg ...
I1107 18:47:27.828077    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1107 18:47:30.344533    7537 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1107 18:47:30.579804    7537 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I1107 18:47:30.593328    7537 api_server.go:140] control plane version: v1.23.3
I1107 18:47:30.593342    7537 api_server.go:130] duration metric: took 4.794230004s to wait for apiserver health ...
I1107 18:47:30.593349    7537 system_pods.go:43] waiting for kube-system pods to appear ...
I1107 18:47:30.593399    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1107 18:47:30.641207    7537 logs.go:274] 2 containers: [49b4ef3c7b74 7cbb8a163e77]
I1107 18:47:30.641281    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1107 18:47:30.692285    7537 logs.go:274] 2 containers: [da4cf9e61ce8 6143cc5693fa]
I1107 18:47:30.692346    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1107 18:47:30.746393    7537 logs.go:274] 2 containers: [b6534c6ee5b0 e9b5d500ebe0]
I1107 18:47:30.746462    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1107 18:47:30.799294    7537 logs.go:274] 2 containers: [1ed89d5b1d77 723efb2ab753]
I1107 18:47:30.799362    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1107 18:47:30.845332    7537 logs.go:274] 1 containers: [e299c4606fa5]
I1107 18:47:30.845392    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I1107 18:47:30.892612    7537 logs.go:274] 2 containers: [66987a5f16e5 95b2c70750d1]
I1107 18:47:30.892677    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1107 18:47:30.927997    7537 logs.go:274] 2 containers: [565aa3f294e0 7eba8c15d7c6]
I1107 18:47:30.928060    7537 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1107 18:47:30.980511    7537 logs.go:274] 2 containers: [5e666a149161 0507240798b3]
I1107 18:47:30.980547    7537 logs.go:123] Gathering logs for kube-scheduler [723efb2ab753] ...
I1107 18:47:30.980557    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 723efb2ab753"
I1107 18:47:31.047532    7537 logs.go:123] Gathering logs for storage-provisioner [565aa3f294e0] ...
I1107 18:47:31.047550    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 565aa3f294e0"
I1107 18:47:31.100014    7537 logs.go:123] Gathering logs for Docker ...
I1107 18:47:31.100033    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I1107 18:47:31.125549    7537 logs.go:123] Gathering logs for container status ...
I1107 18:47:31.125569    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1107 18:47:31.296528    7537 logs.go:123] Gathering logs for kubelet ...
I1107 18:47:31.296552    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1107 18:47:31.383273    7537 logs.go:123] Gathering logs for etcd [6143cc5693fa] ...
I1107 18:47:31.383292    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6143cc5693fa"
I1107 18:47:31.537394    7537 logs.go:123] Gathering logs for kubernetes-dashboard [66987a5f16e5] ...
I1107 18:47:31.537420    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 66987a5f16e5"
I1107 18:47:31.584728    7537 logs.go:123] Gathering logs for kube-controller-manager [5e666a149161] ...
I1107 18:47:31.584762    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5e666a149161"
I1107 18:47:31.642672    7537 logs.go:123] Gathering logs for kube-controller-manager [0507240798b3] ...
I1107 18:47:31.642688    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0507240798b3"
I1107 18:47:31.713210    7537 logs.go:123] Gathering logs for dmesg ...
I1107 18:47:31.713226    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1107 18:47:31.734027    7537 logs.go:123] Gathering logs for coredns [b6534c6ee5b0] ...
I1107 18:47:31.734050    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b6534c6ee5b0"
I1107 18:47:31.778375    7537 logs.go:123] Gathering logs for coredns [e9b5d500ebe0] ...
I1107 18:47:31.778394    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e9b5d500ebe0"
I1107 18:47:31.828397    7537 logs.go:123] Gathering logs for kube-proxy [e299c4606fa5] ...
I1107 18:47:31.828424    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e299c4606fa5"
I1107 18:47:31.868868    7537 logs.go:123] Gathering logs for kube-apiserver [7cbb8a163e77] ...
I1107 18:47:31.868884    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7cbb8a163e77"
I1107 18:47:31.975803    7537 logs.go:123] Gathering logs for etcd [da4cf9e61ce8] ...
I1107 18:47:31.975822    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 da4cf9e61ce8"
I1107 18:47:32.177754    7537 logs.go:123] Gathering logs for kube-scheduler [1ed89d5b1d77] ...
I1107 18:47:32.177777    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1ed89d5b1d77"
I1107 18:47:32.263954    7537 logs.go:123] Gathering logs for kubernetes-dashboard [95b2c70750d1] ...
I1107 18:47:32.263974    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 95b2c70750d1"
W1107 18:47:32.330410    7537 logs.go:130] failed kubernetes-dashboard [95b2c70750d1]: command: /bin/bash -c "docker logs --tail 400 95b2c70750d1" /bin/bash -c "docker logs --tail 400 95b2c70750d1": Process exited with status 1
stdout:

stderr:
Error response from daemon: can not get logs from container which is dead or marked for removal
 output: 
** stderr ** 
Error response from daemon: can not get logs from container which is dead or marked for removal

** /stderr **
I1107 18:47:32.330426    7537 logs.go:123] Gathering logs for storage-provisioner [7eba8c15d7c6] ...
I1107 18:47:32.330435    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7eba8c15d7c6"
I1107 18:47:32.390006    7537 logs.go:123] Gathering logs for describe nodes ...
I1107 18:47:32.390023    7537 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1107 18:47:32.713359    7537 logs.go:123] Gathering logs for kube-apiserver [49b4ef3c7b74] ...
I1107 18:47:32.713382    7537 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 49b4ef3c7b74"
I1107 18:47:35.318613    7537 system_pods.go:59] 7 kube-system pods found
I1107 18:47:35.318640    7537 system_pods.go:61] "coredns-64897985d-frt6z" [e4a702f5-3213-4cb0-b4bd-eceb45dc04af] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1107 18:47:35.318649    7537 system_pods.go:61] "etcd-minikube" [24f782c8-2112-4472-b866-b6caf13582a7] Running
I1107 18:47:35.318658    7537 system_pods.go:61] "kube-apiserver-minikube" [b5c530d8-fad7-447e-8cd2-87fbb6b772b6] Running
I1107 18:47:35.318664    7537 system_pods.go:61] "kube-controller-manager-minikube" [d6d58232-b03d-40cd-a361-c97cb49297a1] Running
I1107 18:47:35.318671    7537 system_pods.go:61] "kube-proxy-ddgh5" [922488aa-2e76-4c96-8e3f-9a69fab97b5a] Running
I1107 18:47:35.318677    7537 system_pods.go:61] "kube-scheduler-minikube" [3d470ed2-a74d-4019-86e8-c5c52e3dec54] Running
I1107 18:47:35.318683    7537 system_pods.go:61] "storage-provisioner" [fad20af1-8fe0-480e-9995-856be38ecdeb] Running
I1107 18:47:35.318690    7537 system_pods.go:74] duration metric: took 4.725336037s to wait for pod list to return data ...
I1107 18:47:35.318701    7537 kubeadm.go:548] duration metric: took 1m39.744993054s to wait for : map[apiserver:true system_pods:true] ...
I1107 18:47:35.318742    7537 node_conditions.go:102] verifying NodePressure condition ...
I1107 18:47:35.322361    7537 node_conditions.go:122] node storage ephemeral capacity is 200475920Ki
I1107 18:47:35.322379    7537 node_conditions.go:123] node cpu capacity is 4
I1107 18:47:35.322389    7537 node_conditions.go:105] duration metric: took 3.639779ms to run NodePressure ...
I1107 18:47:35.322398    7537 start.go:213] waiting for startup goroutines ...
I1107 18:47:35.571065    7537 start.go:496] kubectl: 1.23.5, cluster: 1.23.3 (minor skew: 0)
I1107 18:47:35.630493    7537 out.go:176] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Mon 2022-11-07 11:44:04 UTC, end at Mon 2022-11-07 13:59:04 UTC. --
Nov 07 12:01:41 minikube dockerd[210]: time="2022-11-07T12:01:41.542060190Z" level=info msg="Download failed, retrying (4/5): read tcp 192.168.49.2:60310->104.18.123.25:443: read: connection reset by peer"
Nov 07 12:02:37 minikube dockerd[210]: time="2022-11-07T12:02:37.990817977Z" level=info msg="ignoring event" container=595e60c7b5c5c5b13f2b6048d6c9bccd151a5bac2708c1933931fd0b2eb6e91d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:02:49 minikube dockerd[210]: time="2022-11-07T12:02:49.657603068Z" level=info msg="ignoring event" container=a66fefd1826abc97c8838411da91f759d825520e7dcd1c405bf8f305cab22244 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:06:14 minikube dockerd[210]: time="2022-11-07T12:06:14.257806347Z" level=info msg="Download failed, retrying (5/5): read tcp 192.168.49.2:41300->104.18.125.25:443: read: connection reset by peer"
Nov 07 12:06:32 minikube dockerd[210]: time="2022-11-07T12:06:32.960708343Z" level=info msg="ignoring event" container=1bff141389acec2bbdc8a5f20e14ba362c517ae5de5a1194b70628ddb0f30409 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:06:39 minikube dockerd[210]: time="2022-11-07T12:06:39.991800044Z" level=info msg="ignoring event" container=d15674f8ee818a9496c10867c96c335fd11f4f27c9fa7911a566c44f0d332bf8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:07:07 minikube dockerd[210]: time="2022-11-07T12:07:07.240591969Z" level=info msg="ignoring event" container=bbcd73f1a2889c068b0f2547e08b129d497f1f0b5ce440063c630042aa4b9236 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:07:51 minikube dockerd[210]: time="2022-11-07T12:07:51.341682579Z" level=error msg="Download failed after 6 attempts: read tcp 192.168.49.2:58726->104.18.123.25:443: read: connection reset by peer"
Nov 07 12:07:51 minikube dockerd[210]: time="2022-11-07T12:07:51.447675509Z" level=info msg="Attempting next endpoint for pull after error: read tcp 192.168.49.2:58726->104.18.123.25:443: read: connection reset by peer"
Nov 07 12:07:51 minikube dockerd[210]: time="2022-11-07T12:07:51.573138110Z" level=info msg="Layer sha256:f67dfd6ba00f4cad0ee871cb0b4d2508b9c2096bd2e63aba77857be8ab6d7174 cleaned up"
Nov 07 12:07:51 minikube dockerd[210]: time="2022-11-07T12:07:51.679654424Z" level=info msg="Layer sha256:c2e03b8025877e5347f40bdbd20589662f1b10431759bf0223acc8ef5cfc09ac cleaned up"
Nov 07 12:07:51 minikube dockerd[210]: time="2022-11-07T12:07:51.830225921Z" level=info msg="Layer sha256:f75f864d06d3903602be1b8c202353d06d39d923d838fce2c480abc477d7e6fa cleaned up"
Nov 07 12:07:51 minikube dockerd[210]: time="2022-11-07T12:07:51.830292624Z" level=info msg="Layer sha256:b4ee0f3cb3b63b50b3d20660c662dd7eb7e3a78f164da6259e06e47432407058 cleaned up"
Nov 07 12:07:51 minikube dockerd[210]: time="2022-11-07T12:07:51.938954747Z" level=info msg="Layer sha256:845f4878923e6a7d6d3d6c09eb584bec8adacd6b66205fb4900fb4f044ffb85f cleaned up"
Nov 07 12:08:01 minikube dockerd[210]: time="2022-11-07T12:08:01.325746852Z" level=info msg="Layer sha256:15cfe583cddc2bd7e7208988754dae99dba6d036ffd2d9676652165d4efe63ea cleaned up"
Nov 07 12:08:01 minikube dockerd[210]: time="2022-11-07T12:08:01.325827754Z" level=info msg="Layer sha256:1e5bb34c0db75c9d224dc836d8c47ca6c8e9ac649512e4ba27c39946029148d1 cleaned up"
Nov 07 12:08:01 minikube dockerd[210]: time="2022-11-07T12:08:01.325867910Z" level=info msg="Layer sha256:91a66a7bf89c4806edfb3565833c160f9343f4dd72f1cc6bc94a6db02181ceaa cleaned up"
Nov 07 12:08:01 minikube dockerd[210]: time="2022-11-07T12:08:01.325889867Z" level=info msg="Layer sha256:b29db9b8714e2fbc4e56aeec30bd27df79818c5b22f26a0571eb372888f33531 cleaned up"
Nov 07 12:08:01 minikube dockerd[210]: time="2022-11-07T12:08:01.325911603Z" level=info msg="Layer sha256:89d921d7a33849110fd215af89da0c1d78a799ad7fd16558510e597703b0367b cleaned up"
Nov 07 12:08:01 minikube dockerd[210]: time="2022-11-07T12:08:01.325931356Z" level=info msg="Layer sha256:c40551a3aa81c20a9f1e2953d710a3e413c7389ac7d3d2d73905f217517fad5b cleaned up"
Nov 07 12:08:01 minikube dockerd[210]: time="2022-11-07T12:08:01.325951358Z" level=info msg="Layer sha256:a713da6f4e7c5461fc1a5ef95b6ea369326ac0b1dbe62a76b4f156d75a4b1b1f cleaned up"
Nov 07 12:08:04 minikube dockerd[210]: time="2022-11-07T12:08:04.252011095Z" level=info msg="Layer sha256:d9d07d703dd5ba0b8e23bf7e1bd9f7e4093418a58dc9e470ca013d1c3a1b5bb5 cleaned up"
Nov 07 12:18:08 minikube dockerd[210]: time="2022-11-07T12:18:08.674211563Z" level=info msg="Download failed, retrying (1/5): read tcp 192.168.49.2:46654->104.18.123.25:443: read: connection reset by peer"
Nov 07 12:23:12 minikube dockerd[210]: time="2022-11-07T12:23:12.250346094Z" level=info msg="Download failed, retrying (2/5): read tcp 192.168.49.2:37050->104.18.121.25:443: read: connection reset by peer"
Nov 07 12:25:26 minikube dockerd[210]: time="2022-11-07T12:25:26.710675392Z" level=info msg="Download failed, retrying (3/5): read tcp 192.168.49.2:50924->104.18.124.25:443: read: connection reset by peer"
Nov 07 12:31:11 minikube dockerd[210]: time="2022-11-07T12:31:11.010005330Z" level=info msg="Download failed, retrying (4/5): read tcp 192.168.49.2:57708->104.18.122.25:443: read: connection reset by peer"
Nov 07 12:40:19 minikube dockerd[210]: time="2022-11-07T12:40:19.778908449Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=630af961c6495362412d9c17ee20d1bcb047914eb343eb0130898c683c6d99a5
Nov 07 12:40:20 minikube dockerd[210]: time="2022-11-07T12:40:20.409080240Z" level=info msg="ignoring event" container=630af961c6495362412d9c17ee20d1bcb047914eb343eb0130898c683c6d99a5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:40:20 minikube dockerd[210]: time="2022-11-07T12:40:20.625447127Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=21e87f938c6bc43ba5c6ca716cb957237d4a5790026ddf604b4a0529b923e00d
Nov 07 12:40:21 minikube dockerd[210]: time="2022-11-07T12:40:21.496077839Z" level=info msg="ignoring event" container=21e87f938c6bc43ba5c6ca716cb957237d4a5790026ddf604b4a0529b923e00d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:40:21 minikube dockerd[210]: time="2022-11-07T12:40:21.543825623Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=d4890b7acba30eeffbb4594ce1b0bf0bdcea4667f8c56b285ebe8a543900bca9
Nov 07 12:40:22 minikube dockerd[210]: time="2022-11-07T12:40:22.799622127Z" level=info msg="ignoring event" container=d4890b7acba30eeffbb4594ce1b0bf0bdcea4667f8c56b285ebe8a543900bca9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:40:24 minikube dockerd[210]: time="2022-11-07T12:40:24.266940996Z" level=info msg="ignoring event" container=cf76e88c0be96f3804f71eb451a6d3d58c7f66de82e2275eeda07921aecea66c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:40:24 minikube dockerd[210]: time="2022-11-07T12:40:24.888976607Z" level=info msg="ignoring event" container=76178519a8756da263e8658851e8820c4b57b9f243dbd9902830131c5919a9d5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:40:25 minikube dockerd[210]: time="2022-11-07T12:40:25.338840146Z" level=info msg="ignoring event" container=2c4d5dec738eaafd94148949bdda5744bcf8aa65600465db3c7b27aec81b38fe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:55:33 minikube dockerd[210]: time="2022-11-07T12:55:33.656061347Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=33e33b5c0918d88263c33fe4c26fb304d67509cde991cf697e2cf4734d9c36ca
Nov 07 12:55:33 minikube dockerd[210]: time="2022-11-07T12:55:33.656062359Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=baa092ea1f7cc5be66f0a215a3510cbd90e0c027df7a75b5c54a592d0c751cd2
Nov 07 12:55:34 minikube dockerd[210]: time="2022-11-07T12:55:34.473438385Z" level=info msg="ignoring event" container=33e33b5c0918d88263c33fe4c26fb304d67509cde991cf697e2cf4734d9c36ca module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:55:34 minikube dockerd[210]: time="2022-11-07T12:55:34.474262846Z" level=info msg="ignoring event" container=baa092ea1f7cc5be66f0a215a3510cbd90e0c027df7a75b5c54a592d0c751cd2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:55:37 minikube dockerd[210]: time="2022-11-07T12:55:37.287646205Z" level=info msg="ignoring event" container=4fc0ca624d8ae22c9467009f82cd9d4aef151056fdc1ecfabdf8a5e179d298d7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 12:55:37 minikube dockerd[210]: time="2022-11-07T12:55:37.430393591Z" level=info msg="ignoring event" container=b9688110a0ce10b0a4e3ecc3930ee2ea462185e48b9416832300d4c880d036e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:00:32 minikube dockerd[210]: time="2022-11-07T13:00:32.594294152Z" level=info msg="ignoring event" container=24604d81437e850284e3eecdb92b421b0859c1d9aaf1d56f67b11500b32bab8b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:08:49 minikube dockerd[210]: time="2022-11-07T13:08:49.331866675Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=439723f7b1a33ee187d48497718abe4bd7a8df218c3a37e6b79d42a6653670d6
Nov 07 13:08:49 minikube dockerd[210]: time="2022-11-07T13:08:49.331866685Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=39448d01be4d2274de678e93ed162fe56f6ec1456571d2bbeba1e5a9f8621942
Nov 07 13:08:49 minikube dockerd[210]: time="2022-11-07T13:08:49.805492104Z" level=info msg="ignoring event" container=439723f7b1a33ee187d48497718abe4bd7a8df218c3a37e6b79d42a6653670d6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:08:49 minikube dockerd[210]: time="2022-11-07T13:08:49.852638147Z" level=info msg="ignoring event" container=39448d01be4d2274de678e93ed162fe56f6ec1456571d2bbeba1e5a9f8621942 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:08:53 minikube dockerd[210]: time="2022-11-07T13:08:53.035195697Z" level=info msg="ignoring event" container=4e72f386064d3cbd914514d87797492f82f9917a104bccda33823e6e21e1e900 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:08:53 minikube dockerd[210]: time="2022-11-07T13:08:53.127457302Z" level=info msg="ignoring event" container=160d0e75401e0990b9bc05c6d71df20d2033e150e898939e9fd5469c68ff137b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:23:43 minikube dockerd[210]: time="2022-11-07T13:23:43.536143077Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=7e0bd3e02f487fea0eba9f70a7eb87a5883d5499d14321324e3ebab22b1fed11
Nov 07 13:23:43 minikube dockerd[210]: time="2022-11-07T13:23:43.980502283Z" level=info msg="ignoring event" container=7e0bd3e02f487fea0eba9f70a7eb87a5883d5499d14321324e3ebab22b1fed11 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:23:44 minikube dockerd[210]: time="2022-11-07T13:23:44.949470071Z" level=info msg="ignoring event" container=1da645a05fa77636454f5cd6b549fef82b0d9ee3d605b08036b28d38eaec1bb3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:44:24 minikube dockerd[210]: time="2022-11-07T13:44:24.802598362Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=b50ab5333594b6f6398f3ae89cb135cf749d363d029fc34f3060e3c39a1f7a97
Nov 07 13:44:24 minikube dockerd[210]: time="2022-11-07T13:44:24.802582702Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686
Nov 07 13:44:24 minikube dockerd[210]: time="2022-11-07T13:44:24.802582773Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8
Nov 07 13:44:30 minikube dockerd[210]: time="2022-11-07T13:44:30.836340393Z" level=info msg="ignoring event" container=40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:44:30 minikube dockerd[210]: time="2022-11-07T13:44:30.836853175Z" level=info msg="ignoring event" container=074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:44:30 minikube dockerd[210]: time="2022-11-07T13:44:30.836969605Z" level=info msg="ignoring event" container=b50ab5333594b6f6398f3ae89cb135cf749d363d029fc34f3060e3c39a1f7a97 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:44:35 minikube dockerd[210]: time="2022-11-07T13:44:35.652732061Z" level=info msg="ignoring event" container=6f3d26dfe42c4fbb6a1088078495106cc51b60bddde7b508cb0b962c218d392b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:44:35 minikube dockerd[210]: time="2022-11-07T13:44:35.704939699Z" level=info msg="ignoring event" container=6415fe2b6865d281546c35091b8b05b72c4d71f9efd7b638921a3b2f63a1a42a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 07 13:44:35 minikube dockerd[210]: time="2022-11-07T13:44:35.705436371Z" level=info msg="ignoring event" container=9f7501ac4fd10f9fa3197e7f04804fc793e70023e40e9d544db1c787dc782dd6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID
08a556aed9646       0679cc1c79a68       14 minutes ago      Running             sdp-app                     0                   8e504395289e2
fc2c87ebca197       0679cc1c79a68       14 minutes ago      Running             sdp-app                     0                   2d8be2a3b570d
121dca960321a       0679cc1c79a68       14 minutes ago      Running             sdp-app                     0                   ca060d0d0f481
d29730e8928f4       e1482a24335a6       2 hours ago         Running             kubernetes-dashboard        8                   2f135c9340142
565aa3f294e01       6e38f40d628db       2 hours ago         Running             storage-provisioner         14                  f634596524228
66987a5f16e5d       e1482a24335a6       2 hours ago         Exited              kubernetes-dashboard        7                   2f135c9340142
b6534c6ee5b0e       a4ca41631cc7a       2 hours ago         Running             coredns                     6                   590b75463923f
764916a6634be       7801cfc6d5c07       2 hours ago         Running             dashboard-metrics-scraper   4                   752bdc976b3b5
e299c4606fa50       9b7cc99821098       2 hours ago         Running             kube-proxy                  6                   cbca9eb8c8432
7eba8c15d7c68       6e38f40d628db       2 hours ago         Exited              storage-provisioner         13                  f634596524228
da4cf9e61ce8f       25f8c7f3da61c       2 hours ago         Running             etcd                        6                   07f199e3d2fe0
5e666a1491616       b07520cd7ab76       2 hours ago         Running             kube-controller-manager     7                   278cd67a4d48d
49b4ef3c7b741       f40be0088a83e       2 hours ago         Running             kube-apiserver              6                   4847f84bd1d81
1ed89d5b1d77b       99a3486be4f28       2 hours ago         Running             kube-scheduler              6                   feb15b554195a
250d5c8078cfb       7801cfc6d5c07       4 days ago          Exited              dashboard-metrics-scraper   3                   b99605b20d6b5
e9b5d500ebe0c       a4ca41631cc7a       4 days ago          Exited              coredns                     5                   604bf93ed185a
7cbb8a163e778       f40be0088a83e       4 days ago          Exited              kube-apiserver              5                   0b019d8f66f9f
723efb2ab7539       99a3486be4f28       4 days ago          Exited              kube-scheduler              5                   ab091607a2412
0507240798b34       b07520cd7ab76       4 days ago          Exited              kube-controller-manager     6                   bda45d603eced
6143cc5693fa7       25f8c7f3da61c       4 days ago          Exited              etcd                        5                   99440876db1b0

* 
* ==> coredns [b6534c6ee5b0] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [e9b5d500ebe0] <==
* [WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_03_27T09_32_00_0700
                    minikube.k8s.io/version=v1.25.2
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 27 Mar 2022 02:31:56 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 07 Nov 2022 13:59:13 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 07 Nov 2022 13:56:23 +0000   Sun, 27 Mar 2022 02:31:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 07 Nov 2022 13:56:23 +0000   Sun, 27 Mar 2022 02:31:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 07 Nov 2022 13:56:23 +0000   Sun, 27 Mar 2022 02:31:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 07 Nov 2022 13:56:23 +0000   Sun, 27 Mar 2022 02:32:11 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  200475920Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             6022280Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  200475920Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             6022280Ki
  pods:               110
System Info:
  Machine ID:                 b6a262faae404a5db719705fd34b5c8b
  System UUID:                58eccbe3-86b4-479f-9125-0a8057b16735
  Boot ID:                    08659322-6d2a-4404-8fd0-9ab3b15fdb52
  Kernel Version:             5.15.0-52-generic
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.12
  Kubelet Version:            v1.23.3
  Kube-Proxy Version:         v1.23.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     sdp-app-5856659d95-55cs7                     250m (6%!)(MISSING)     250m (6%!)(MISSING)   512Mi (8%!)(MISSING)       512Mi (8%!)(MISSING)     15m
  default                     sdp-app-5856659d95-npds4                     250m (6%!)(MISSING)     250m (6%!)(MISSING)   512Mi (8%!)(MISSING)       512Mi (8%!)(MISSING)     15m
  default                     sdp-app-5856659d95-vrvh5                     250m (6%!)(MISSING)     250m (6%!)(MISSING)   512Mi (8%!)(MISSING)       512Mi (8%!)(MISSING)     15m
  kube-system                 coredns-64897985d-frt6z                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     225d
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         225d
  kube-system                 kube-apiserver-minikube                      250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         225d
  kube-system                 kube-controller-manager-minikube             200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         225d
  kube-system                 kube-proxy-ddgh5                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         225d
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         225d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         225d
  kubernetes-dashboard        dashboard-metrics-scraper-58549894f-dsjgc    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         222d
  kubernetes-dashboard        kubernetes-dashboard-ccd587f44-7mwwg         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         222d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1500m (37%!)(MISSING)   750m (18%!)(MISSING)
  memory             1706Mi (29%!)(MISSING)  1706Mi (29%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:
  Type     Reason                                            Age   From        Message
  ----     ------                                            ----  ----        -------
  Warning  listen tcp4 :31743: bind: address already in use  46m   kube-proxy  can't open port "nodePort for default/sdp-app" (:31743/tcp4), skipping it
  Warning  listen tcp4 :32265: bind: address already in use  18m   kube-proxy  can't open port "nodePort for default/sdp-app" (:32265/tcp4), skipping it
  Warning  listen tcp4 :31574: bind: address already in use  15m   kube-proxy  can't open port "nodePort for default/sdp-app" (:31574/tcp4), skipping it

* 
* ==> dmesg <==
* [Nov 7 11:20] Expanded resource Reserved due to conflict with PCI Bus 0000:00
[  +0.018563] pci 0000:00:00.2: can't derive routing for PCI INT A
[  +0.000001] pci 0000:00:00.2: PCI INT A: not connected
[  +0.089992] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.004927] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000119] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.387157] acpi PNP0C14:01: duplicate WMI GUID 05901221-D566-11D1-B2F0-00A0C9062910 (first instance was on PNP0C14:00)
[  +0.000113] acpi PNP0C14:02: duplicate WMI GUID 05901221-D566-11D1-B2F0-00A0C9062910 (first instance was on PNP0C14:00)
[  +0.096007] nvme nvme0: missing or invalid SUBNQN field.
[Nov 7 11:21] r8188eu: module is from the staging directory, the quality is unknown, you have been warned.
[  +0.962328] thermal thermal_zone1: failed to read out thermal zone (-61)
[  +1.326012] amdgpu 0000:03:00.0: amdgpu: PSP runtime database doesn't exist
[  +0.189544] [drm] failed to load ucode RLC_RESTORE_LIST_CNTL(0x11) 
[  +0.000005] [drm] psp gfx command LOAD_IP_FW(0x6) failed and response status is (0xFFFF300F)
[  +0.000627] [drm] failed to load ucode RLC_RESTORE_LIST_GPM_MEM(0x12) 
[  +0.000002] [drm] psp gfx command LOAD_IP_FW(0x6) failed and response status is (0xFFFF000F)
[  +0.000423] [drm] failed to load ucode RLC_RESTORE_LIST_SRM_MEM(0x13) 
[  +0.000002] [drm] psp gfx command LOAD_IP_FW(0x6) failed and response status is (0xFFFF000F)
[  +0.319062] Bluetooth: hci0: Reading supported features failed (-16)
[ +18.128899] kauditd_printk_skb: 27 callbacks suppressed
[Nov 7 11:25] show_signal_msg: 1 callbacks suppressed
[Nov 7 11:38] hid-generic 0005:05AC:024F.0003: unknown main item tag 0x0
[  +0.100590] apple 0005:05AC:024F.0003: unknown main item tag 0x0
[ +19.615705] apple 0005:05AC:024F.0004: unknown main item tag 0x0
[Nov 7 12:28] show_signal_msg: 1 callbacks suppressed
[Nov 7 13:32] apple 0005:05AC:024F.0005: unknown main item tag 0x0

* 
* ==> etcd [6143cc5693fa] <==
* {"level":"warn","ts":"2022-11-03T13:51:43.678Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:42.792Z","time spent":"885.504036ms","remote":"127.0.0.1:52872","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":4,"response size":32,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true "}
{"level":"warn","ts":"2022-11-03T13:51:43.678Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"767.075468ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-03T13:51:43.678Z","caller":"traceutil/trace.go:171","msg":"trace[1369216694] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:61687; }","duration":"767.129851ms","start":"2022-11-03T13:51:42.911Z","end":"2022-11-03T13:51:43.678Z","steps":["trace[1369216694] 'agreement among raft nodes before linearized reading'  (duration: 767.035713ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:51:43.678Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:42.911Z","time spent":"767.222857ms","remote":"127.0.0.1:52432","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":12,"response size":32,"request content":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true "}
{"level":"warn","ts":"2022-11-03T13:51:44.516Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"319.492833ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2022-11-03T13:51:44.517Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"465.89664ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2022-11-03T13:51:44.517Z","caller":"traceutil/trace.go:171","msg":"trace[1026737567] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:61687; }","duration":"465.977624ms","start":"2022-11-03T13:51:44.051Z","end":"2022-11-03T13:51:44.517Z","steps":["trace[1026737567] 'range keys from in-memory index tree'  (duration: 465.720467ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:51:44.517Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"697.901852ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/system-nodes\" ","response":"range_response_count:1 size:1082"}
{"level":"info","ts":"2022-11-03T13:51:44.517Z","caller":"traceutil/trace.go:171","msg":"trace[1189255715] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:61687; }","duration":"319.653738ms","start":"2022-11-03T13:51:44.197Z","end":"2022-11-03T13:51:44.517Z","steps":["trace[1189255715] 'range keys from in-memory index tree'  (duration: 319.360072ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:51:44.517Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:44.051Z","time spent":"466.078063ms","remote":"127.0.0.1:52412","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1138,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2022-11-03T13:51:44.517Z","caller":"traceutil/trace.go:171","msg":"trace[1252885127] range","detail":"{range_begin:/registry/flowschemas/system-nodes; range_end:; response_count:1; response_revision:61687; }","duration":"698.365629ms","start":"2022-11-03T13:51:43.819Z","end":"2022-11-03T13:51:44.517Z","steps":["trace[1252885127] 'range keys from in-memory index tree'  (duration: 697.5887ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:51:44.517Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:43.818Z","time spent":"698.707875ms","remote":"127.0.0.1:52764","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":1,"response size":1106,"request content":"key:\"/registry/flowschemas/system-nodes\" "}
{"level":"info","ts":"2022-11-03T13:51:44.941Z","caller":"traceutil/trace.go:171","msg":"trace[1828314883] linearizableReadLoop","detail":"{readStateIndex:79233; appliedIndex:79233; }","duration":"407.144085ms","start":"2022-11-03T13:51:44.534Z","end":"2022-11-03T13:51:44.941Z","steps":["trace[1828314883] 'read index received'  (duration: 407.130729ms)","trace[1828314883] 'applied index is now lower than readState.Index'  (duration: 11.272¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:51:45.074Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"365.448536ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2022-11-03T13:51:45.075Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"541.002793ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/system-leader-election\" ","response":"range_response_count:1 size:1303"}
{"level":"info","ts":"2022-11-03T13:51:45.074Z","caller":"traceutil/trace.go:171","msg":"trace[1009832607] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:61688; }","duration":"365.613919ms","start":"2022-11-03T13:51:44.709Z","end":"2022-11-03T13:51:45.074Z","steps":["trace[1009832607] 'agreement among raft nodes before linearized reading'  (duration: 232.305019ms)","trace[1009832607] 'range keys from in-memory index tree'  (duration: 133.112839ms)"],"step_count":2}
{"level":"info","ts":"2022-11-03T13:51:45.075Z","caller":"traceutil/trace.go:171","msg":"trace[513990092] range","detail":"{range_begin:/registry/flowschemas/system-leader-election; range_end:; response_count:1; response_revision:61688; }","duration":"541.078486ms","start":"2022-11-03T13:51:44.534Z","end":"2022-11-03T13:51:45.075Z","steps":["trace[513990092] 'agreement among raft nodes before linearized reading'  (duration: 407.280582ms)","trace[513990092] 'range keys from in-memory index tree'  (duration: 133.308369ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:51:45.075Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:44.534Z","time spent":"541.242256ms","remote":"127.0.0.1:52764","response type":"/etcdserverpb.KV/Range","request count":0,"request size":46,"response count":1,"response size":1327,"request content":"key:\"/registry/flowschemas/system-leader-election\" "}
{"level":"warn","ts":"2022-11-03T13:51:45.075Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:44.709Z","time spent":"366.038191ms","remote":"127.0.0.1:52380","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2022-11-03T13:51:46.674Z","caller":"traceutil/trace.go:171","msg":"trace[479882970] linearizableReadLoop","detail":"{readStateIndex:79234; appliedIndex:79234; }","duration":"101.78439ms","start":"2022-11-03T13:51:46.573Z","end":"2022-11-03T13:51:46.674Z","steps":["trace[479882970] 'read index received'  (duration: 101.77359ms)","trace[479882970] 'applied index is now lower than readState.Index'  (duration: 9.308¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:51:46.968Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"343.625768ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-03T13:51:46.968Z","caller":"traceutil/trace.go:171","msg":"trace[958416570] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:61689; }","duration":"343.738681ms","start":"2022-11-03T13:51:46.624Z","end":"2022-11-03T13:51:46.968Z","steps":["trace[958416570] 'agreement among raft nodes before linearized reading'  (duration: 50.532575ms)","trace[958416570] 'range keys from in-memory index tree'  (duration: 293.071262ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:51:46.968Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"395.130331ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/servicenodeports\" ","response":"range_response_count:1 size:123"}
{"level":"warn","ts":"2022-11-03T13:51:46.968Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:46.624Z","time spent":"343.87543ms","remote":"127.0.0.1:52380","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2022-11-03T13:51:46.968Z","caller":"traceutil/trace.go:171","msg":"trace[677121023] range","detail":"{range_begin:/registry/ranges/servicenodeports; range_end:; response_count:1; response_revision:61689; }","duration":"395.366958ms","start":"2022-11-03T13:51:46.572Z","end":"2022-11-03T13:51:46.968Z","steps":["trace[677121023] 'agreement among raft nodes before linearized reading'  (duration: 101.946457ms)","trace[677121023] 'range keys from in-memory index tree'  (duration: 293.123309ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:51:46.968Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:46.572Z","time spent":"395.469362ms","remote":"127.0.0.1:52458","response type":"/etcdserverpb.KV/Range","request count":0,"request size":35,"response count":1,"response size":147,"request content":"key:\"/registry/ranges/servicenodeports\" "}
{"level":"warn","ts":"2022-11-03T13:51:46.968Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"259.758071ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-03T13:51:46.968Z","caller":"traceutil/trace.go:171","msg":"trace[1506205951] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:61689; }","duration":"260.420974ms","start":"2022-11-03T13:51:46.708Z","end":"2022-11-03T13:51:46.968Z","steps":["trace[1506205951] 'range keys from in-memory index tree'  (duration: 259.559806ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:51:48.042Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"333.353779ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2022-11-03T13:51:48.042Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"266.222262ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" ","response":"range_response_count:1 size:62934"}
{"level":"info","ts":"2022-11-03T13:51:48.042Z","caller":"traceutil/trace.go:171","msg":"trace[1106733787] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:61689; }","duration":"266.271124ms","start":"2022-11-03T13:51:47.776Z","end":"2022-11-03T13:51:48.042Z","steps":["trace[1106733787] 'range keys from in-memory index tree'  (duration: 265.945859ms)"],"step_count":1}
{"level":"info","ts":"2022-11-03T13:51:48.042Z","caller":"traceutil/trace.go:171","msg":"trace[526537612] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:61689; }","duration":"333.489155ms","start":"2022-11-03T13:51:47.709Z","end":"2022-11-03T13:51:48.042Z","steps":["trace[526537612] 'agreement among raft nodes before linearized reading'  (duration: 33.262575ms)","trace[526537612] 'range keys from in-memory index tree'  (duration: 300.061608ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:51:48.042Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:51:47.709Z","time spent":"333.585507ms","remote":"127.0.0.1:52380","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-03T13:51:48.976Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"268.118897ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-03T13:51:48.976Z","caller":"traceutil/trace.go:171","msg":"trace[829652408] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:61690; }","duration":"268.25783ms","start":"2022-11-03T13:51:48.708Z","end":"2022-11-03T13:51:48.976Z","steps":["trace[829652408] 'range keys from in-memory index tree'  (duration: 256.241192ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:51:50.903Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"194.617498ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-03T13:51:50.903Z","caller":"traceutil/trace.go:171","msg":"trace[1316529335] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:61692; }","duration":"194.749136ms","start":"2022-11-03T13:51:50.708Z","end":"2022-11-03T13:51:50.903Z","steps":["trace[1316529335] 'range keys from in-memory index tree'  (duration: 194.47054ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:51:52.717Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"168.001167ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016816729807680 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:61687 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:68 lease:8128016816729807678 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-03T13:51:52.717Z","caller":"traceutil/trace.go:171","msg":"trace[933516001] transaction","detail":"{read_only:false; response_revision:61693; number_of_response:1; }","duration":"237.24591ms","start":"2022-11-03T13:51:52.480Z","end":"2022-11-03T13:51:52.717Z","steps":["trace[933516001] 'process raft request'  (duration: 68.996824ms)","trace[933516001] 'compare'  (duration: 167.851174ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:52:02.747Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"189.462401ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016816729807724 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:61693 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:68 lease:8128016816729807722 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-03T13:52:02.748Z","caller":"traceutil/trace.go:171","msg":"trace[787551070] linearizableReadLoop","detail":"{readStateIndex:79249; appliedIndex:79248; }","duration":"206.290737ms","start":"2022-11-03T13:52:02.541Z","end":"2022-11-03T13:52:02.748Z","steps":["trace[787551070] 'read index received'  (duration: 16.56096ms)","trace[787551070] 'applied index is now lower than readState.Index'  (duration: 189.727543ms)"],"step_count":2}
{"level":"info","ts":"2022-11-03T13:52:02.748Z","caller":"traceutil/trace.go:171","msg":"trace[1292245858] transaction","detail":"{read_only:false; response_revision:61700; number_of_response:1; }","duration":"268.268621ms","start":"2022-11-03T13:52:02.479Z","end":"2022-11-03T13:52:02.748Z","steps":["trace[1292245858] 'process raft request'  (duration: 78.579391ms)","trace[1292245858] 'compare'  (duration: 189.337575ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:52:02.748Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"206.533036ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-03T13:52:02.748Z","caller":"traceutil/trace.go:171","msg":"trace[235088264] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:61700; }","duration":"206.592989ms","start":"2022-11-03T13:52:02.541Z","end":"2022-11-03T13:52:02.748Z","steps":["trace[235088264] 'agreement among raft nodes before linearized reading'  (duration: 206.460689ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:52:03.315Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"466.203119ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016816729807731 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:61699 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-03T13:52:03.315Z","caller":"traceutil/trace.go:171","msg":"trace[765120268] linearizableReadLoop","detail":"{readStateIndex:79250; appliedIndex:79249; }","duration":"186.0389ms","start":"2022-11-03T13:52:03.129Z","end":"2022-11-03T13:52:03.315Z","steps":["trace[765120268] 'read index received'  (duration: 7.984776ms)","trace[765120268] 'applied index is now lower than readState.Index'  (duration: 178.052601ms)"],"step_count":2}
{"level":"info","ts":"2022-11-03T13:52:03.315Z","caller":"traceutil/trace.go:171","msg":"trace[1852985236] transaction","detail":"{read_only:false; response_revision:61701; number_of_response:1; }","duration":"466.594049ms","start":"2022-11-03T13:52:02.849Z","end":"2022-11-03T13:52:03.315Z","steps":["trace[1852985236] 'compare'  (duration: 466.036324ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:52:03.315Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"186.182801ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2022-11-03T13:52:03.317Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:52:02.849Z","time spent":"466.744253ms","remote":"127.0.0.1:52412","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1095,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:61699 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1022 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2022-11-03T13:52:03.316Z","caller":"traceutil/trace.go:171","msg":"trace[1182033151] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:61701; }","duration":"186.234169ms","start":"2022-11-03T13:52:03.129Z","end":"2022-11-03T13:52:03.315Z","steps":["trace[1182033151] 'agreement among raft nodes before linearized reading'  (duration: 186.152414ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:52:12.811Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"255.094144ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:425"}
{"level":"info","ts":"2022-11-03T13:52:12.811Z","caller":"traceutil/trace.go:171","msg":"trace[1435793053] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:61706; }","duration":"255.227416ms","start":"2022-11-03T13:52:12.556Z","end":"2022-11-03T13:52:12.811Z","steps":["trace[1435793053] 'agreement among raft nodes before linearized reading'  (duration: 87.665257ms)","trace[1435793053] 'range keys from in-memory index tree'  (duration: 167.369855ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-03T13:52:13.978Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"224.653373ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-03T13:52:13.978Z","caller":"traceutil/trace.go:171","msg":"trace[1076497839] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:61707; }","duration":"224.794971ms","start":"2022-11-03T13:52:13.753Z","end":"2022-11-03T13:52:13.978Z","steps":["trace[1076497839] 'count revisions from in-memory index tree'  (duration: 224.483352ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:52:17.916Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"189.213021ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016816729807766 > lease_revoke:<id:70cc843b1194db6a>","response":"size:30"}
{"level":"info","ts":"2022-11-03T13:52:20.665Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-11-03T13:52:22.519Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2022-11-03T13:52:23.917Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"529.25232ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-03T13:52:23.917Z","caller":"traceutil/trace.go:171","msg":"trace[835104193] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:61710; }","duration":"529.401532ms","start":"2022-11-03T13:52:23.388Z","end":"2022-11-03T13:52:23.917Z","steps":["trace[835104193] 'count revisions from in-memory index tree'  (duration: 529.075395ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-03T13:52:23.917Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-03T13:52:23.388Z","time spent":"529.501671ms","remote":"127.0.0.1:52628","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":0,"response size":30,"request content":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true "}

* 
* ==> etcd [da4cf9e61ce8] <==
* {"level":"info","ts":"2022-11-07T13:55:51.258Z","caller":"traceutil/trace.go:171","msg":"trace[1712974052] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:68015; }","duration":"122.591042ms","start":"2022-11-07T13:55:51.135Z","end":"2022-11-07T13:55:51.258Z","steps":["trace[1712974052] 'agreement among raft nodes before linearized reading'  (duration: 122.36457ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:55:51.258Z","caller":"traceutil/trace.go:171","msg":"trace[1337159541] compact","detail":"{revision:67806; response_revision:68015; }","duration":"173.233256ms","start":"2022-11-07T13:55:51.085Z","end":"2022-11-07T13:55:51.258Z","steps":["trace[1337159541] 'process raft request'  (duration: 41.466135ms)","trace[1337159541] 'check and update compact revision'  (duration: 100.3927ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:55:51.228Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":67806,"took":"788.389¬µs"}
{"level":"warn","ts":"2022-11-07T13:56:09.559Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"132.566019ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/\" range_end:\"/registry/endpointslices0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2022-11-07T13:56:09.559Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"115.355036ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2022-11-07T13:56:09.559Z","caller":"traceutil/trace.go:171","msg":"trace[1822318425] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:68029; }","duration":"115.807228ms","start":"2022-11-07T13:56:09.444Z","end":"2022-11-07T13:56:09.559Z","steps":["trace[1822318425] 'agreement among raft nodes before linearized reading'  (duration: 23.780064ms)","trace[1822318425] 'range keys from in-memory index tree'  (duration: 91.535908ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:56:09.559Z","caller":"traceutil/trace.go:171","msg":"trace[342798111] range","detail":"{range_begin:/registry/endpointslices/; range_end:/registry/endpointslices0; response_count:0; response_revision:68029; }","duration":"132.737758ms","start":"2022-11-07T13:56:09.426Z","end":"2022-11-07T13:56:09.559Z","steps":["trace[342798111] 'agreement among raft nodes before linearized reading'  (duration: 40.995987ms)","trace[342798111] 'count revisions from in-memory index tree'  (duration: 91.540347ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:56:11.695Z","caller":"traceutil/trace.go:171","msg":"trace[966139325] transaction","detail":"{read_only:false; response_revision:68031; number_of_response:1; }","duration":"105.330748ms","start":"2022-11-07T13:56:11.589Z","end":"2022-11-07T13:56:11.695Z","steps":["trace[966139325] 'process raft request'  (duration: 105.175739ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:56:19.720Z","caller":"traceutil/trace.go:171","msg":"trace[987409286] linearizableReadLoop","detail":"{readStateIndex:87281; appliedIndex:87281; }","duration":"127.90988ms","start":"2022-11-07T13:56:19.592Z","end":"2022-11-07T13:56:19.720Z","steps":["trace[987409286] 'read index received'  (duration: 127.895543ms)","trace[987409286] 'applied index is now lower than readState.Index'  (duration: 12.243¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:56:19.742Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"150.652665ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:56:19.742Z","caller":"traceutil/trace.go:171","msg":"trace[349243711] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:68036; }","duration":"150.789911ms","start":"2022-11-07T13:56:19.592Z","end":"2022-11-07T13:56:19.742Z","steps":["trace[349243711] 'agreement among raft nodes before linearized reading'  (duration: 128.047566ms)","trace[349243711] 'count revisions from in-memory index tree'  (duration: 22.573008ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:56:33.022Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"134.251887ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016914834978820 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:68039 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:68 lease:8128016914834978818 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-07T13:56:33.022Z","caller":"traceutil/trace.go:171","msg":"trace[1870014757] transaction","detail":"{read_only:false; response_revision:68047; number_of_response:1; }","duration":"238.398721ms","start":"2022-11-07T13:56:32.784Z","end":"2022-11-07T13:56:33.022Z","steps":["trace[1870014757] 'process raft request'  (duration: 103.918938ms)","trace[1870014757] 'compare'  (duration: 134.100095ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:56:42.986Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"109.378924ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:425"}
{"level":"info","ts":"2022-11-07T13:56:42.986Z","caller":"traceutil/trace.go:171","msg":"trace[206152378] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:68054; }","duration":"109.507774ms","start":"2022-11-07T13:56:42.876Z","end":"2022-11-07T13:56:42.986Z","steps":["trace[206152378] 'agreement among raft nodes before linearized reading'  (duration: 53.902333ms)","trace[206152378] 'range keys from in-memory index tree'  (duration: 55.429272ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:56:52.511Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"115.219125ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:56:52.512Z","caller":"traceutil/trace.go:171","msg":"trace[246862164] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:68060; }","duration":"115.351001ms","start":"2022-11-07T13:56:52.396Z","end":"2022-11-07T13:56:52.511Z","steps":["trace[246862164] 'count revisions from in-memory index tree'  (duration: 111.631991ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:56:52.912Z","caller":"traceutil/trace.go:171","msg":"trace[1603779842] transaction","detail":"{read_only:false; response_revision:68061; number_of_response:1; }","duration":"122.328872ms","start":"2022-11-07T13:56:52.790Z","end":"2022-11-07T13:56:52.912Z","steps":["trace[1603779842] 'process raft request'  (duration: 99.773803ms)","trace[1603779842] 'compare'  (duration: 22.429955ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:56:54.581Z","caller":"traceutil/trace.go:171","msg":"trace[1436966613] linearizableReadLoop","detail":"{readStateIndex:87314; appliedIndex:87314; }","duration":"106.344917ms","start":"2022-11-07T13:56:54.474Z","end":"2022-11-07T13:56:54.581Z","steps":["trace[1436966613] 'read index received'  (duration: 106.336141ms)","trace[1436966613] 'applied index is now lower than readState.Index'  (duration: 7.294¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:56:54.581Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.500748ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:56:54.581Z","caller":"traceutil/trace.go:171","msg":"trace[111017754] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:68062; }","duration":"106.554629ms","start":"2022-11-07T13:56:54.474Z","end":"2022-11-07T13:56:54.581Z","steps":["trace[111017754] 'agreement among raft nodes before linearized reading'  (duration: 106.466935ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:57:02.605Z","caller":"traceutil/trace.go:171","msg":"trace[2079074088] transaction","detail":"{read_only:false; response_revision:68067; number_of_response:1; }","duration":"119.215048ms","start":"2022-11-07T13:57:02.486Z","end":"2022-11-07T13:57:02.605Z","steps":["trace[2079074088] 'process raft request'  (duration: 119.061722ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:57:02.976Z","caller":"traceutil/trace.go:171","msg":"trace[835627081] transaction","detail":"{read_only:false; response_revision:68068; number_of_response:1; }","duration":"186.109835ms","start":"2022-11-07T13:57:02.790Z","end":"2022-11-07T13:57:02.976Z","steps":["trace[835627081] 'process raft request'  (duration: 163.54776ms)","trace[835627081] 'compare'  (duration: 22.389213ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:57:10.582Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.686412ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:57:10.582Z","caller":"traceutil/trace.go:171","msg":"trace[1389906334] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:68072; }","duration":"105.75531ms","start":"2022-11-07T13:57:10.476Z","end":"2022-11-07T13:57:10.582Z","steps":["trace[1389906334] 'agreement among raft nodes before linearized reading'  (duration: 94.788765ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:57:12.864Z","caller":"traceutil/trace.go:171","msg":"trace[1441520735] linearizableReadLoop","detail":"{readStateIndex:87329; appliedIndex:87329; }","duration":"105.936651ms","start":"2022-11-07T13:57:12.758Z","end":"2022-11-07T13:57:12.864Z","steps":["trace[1441520735] 'read index received'  (duration: 105.924719ms)","trace[1441520735] 'applied index is now lower than readState.Index'  (duration: 10.189¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:57:12.864Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.109684ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:57:12.864Z","caller":"traceutil/trace.go:171","msg":"trace[1953991654] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:68074; }","duration":"106.220802ms","start":"2022-11-07T13:57:12.758Z","end":"2022-11-07T13:57:12.864Z","steps":["trace[1953991654] 'agreement among raft nodes before linearized reading'  (duration: 106.054321ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:57:18.907Z","caller":"traceutil/trace.go:171","msg":"trace[1148862772] linearizableReadLoop","detail":"{readStateIndex:87335; appliedIndex:87335; }","duration":"113.996677ms","start":"2022-11-07T13:57:18.793Z","end":"2022-11-07T13:57:18.907Z","steps":["trace[1148862772] 'read index received'  (duration: 113.986889ms)","trace[1148862772] 'applied index is now lower than readState.Index'  (duration: 7.975¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:57:18.930Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"136.747183ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:895"}
{"level":"info","ts":"2022-11-07T13:57:18.930Z","caller":"traceutil/trace.go:171","msg":"trace[537229240] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:68078; }","duration":"136.8204ms","start":"2022-11-07T13:57:18.793Z","end":"2022-11-07T13:57:18.930Z","steps":["trace[537229240] 'agreement among raft nodes before linearized reading'  (duration: 114.119306ms)","trace[537229240] 'range keys from in-memory index tree'  (duration: 22.574327ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:57:20.954Z","caller":"traceutil/trace.go:171","msg":"trace[484686652] linearizableReadLoop","detail":"{readStateIndex:87336; appliedIndex:87336; }","duration":"167.215541ms","start":"2022-11-07T13:57:20.786Z","end":"2022-11-07T13:57:20.954Z","steps":["trace[484686652] 'read index received'  (duration: 167.202637ms)","trace[484686652] 'applied index is now lower than readState.Index'  (duration: 11.321¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:57:20.954Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"167.435943ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2022-11-07T13:57:20.954Z","caller":"traceutil/trace.go:171","msg":"trace[1553603641] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:68079; }","duration":"167.518427ms","start":"2022-11-07T13:57:20.786Z","end":"2022-11-07T13:57:20.954Z","steps":["trace[1553603641] 'agreement among raft nodes before linearized reading'  (duration: 167.339542ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:57:31.258Z","caller":"traceutil/trace.go:171","msg":"trace[1304078096] linearizableReadLoop","detail":"{readStateIndex:87345; appliedIndex:87345; }","duration":"177.261185ms","start":"2022-11-07T13:57:31.080Z","end":"2022-11-07T13:57:31.258Z","steps":["trace[1304078096] 'read index received'  (duration: 177.251337ms)","trace[1304078096] 'applied index is now lower than readState.Index'  (duration: 8.255¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:57:31.258Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"177.468863ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2022-11-07T13:57:31.258Z","caller":"traceutil/trace.go:171","msg":"trace[338217211] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:68086; }","duration":"177.55798ms","start":"2022-11-07T13:57:31.080Z","end":"2022-11-07T13:57:31.258Z","steps":["trace[338217211] 'agreement among raft nodes before linearized reading'  (duration: 177.383073ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:57:32.953Z","caller":"traceutil/trace.go:171","msg":"trace[1091654317] transaction","detail":"{read_only:false; response_revision:68088; number_of_response:1; }","duration":"160.962585ms","start":"2022-11-07T13:57:32.792Z","end":"2022-11-07T13:57:32.953Z","steps":["trace[1091654317] 'process raft request'  (duration: 138.388762ms)","trace[1091654317] 'compare'  (duration: 22.442979ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:57:42.907Z","caller":"traceutil/trace.go:171","msg":"trace[1961959700] transaction","detail":"{read_only:false; response_revision:68095; number_of_response:1; }","duration":"114.602443ms","start":"2022-11-07T13:57:42.792Z","end":"2022-11-07T13:57:42.907Z","steps":["trace[1961959700] 'process raft request'  (duration: 92.021094ms)","trace[1961959700] 'compare'  (duration: 22.472385ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:57:49.982Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"113.724684ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-key-holder\" ","response":"range_response_count:1 size:3261"}
{"level":"info","ts":"2022-11-07T13:57:49.982Z","caller":"traceutil/trace.go:171","msg":"trace[944730702] range","detail":"{range_begin:/registry/secrets/kubernetes-dashboard/kubernetes-dashboard-key-holder; range_end:; response_count:1; response_revision:68099; }","duration":"113.836663ms","start":"2022-11-07T13:57:49.868Z","end":"2022-11-07T13:57:49.982Z","steps":["trace[944730702] 'agreement among raft nodes before linearized reading'  (duration: 91.027169ms)","trace[944730702] 'range keys from in-memory index tree'  (duration: 22.644104ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:57:52.929Z","caller":"traceutil/trace.go:171","msg":"trace[1280779353] transaction","detail":"{read_only:false; response_revision:68102; number_of_response:1; }","duration":"134.915492ms","start":"2022-11-07T13:57:52.794Z","end":"2022-11-07T13:57:52.929Z","steps":["trace[1280779353] 'process raft request'  (duration: 112.388433ms)","trace[1280779353] 'compare'  (duration: 22.385324ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:58:12.941Z","caller":"traceutil/trace.go:171","msg":"trace[1932428030] transaction","detail":"{read_only:false; response_revision:68116; number_of_response:1; }","duration":"147.37692ms","start":"2022-11-07T13:58:12.793Z","end":"2022-11-07T13:58:12.941Z","steps":["trace[1932428030] 'process raft request'  (duration: 112.767052ms)","trace[1932428030] 'compare'  (duration: 34.446142ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:58:16.481Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"141.088246ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-07T13:58:16.481Z","caller":"traceutil/trace.go:171","msg":"trace[424780131] range","detail":"{range_begin:/registry/rolebindings/; range_end:/registry/rolebindings0; response_count:0; response_revision:68118; }","duration":"141.22945ms","start":"2022-11-07T13:58:16.340Z","end":"2022-11-07T13:58:16.481Z","steps":["trace[424780131] 'agreement among raft nodes before linearized reading'  (duration: 99.597861ms)","trace[424780131] 'count revisions from in-memory index tree'  (duration: 41.465769ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:58:24.584Z","caller":"traceutil/trace.go:171","msg":"trace[1712520298] linearizableReadLoop","detail":"{readStateIndex:87394; appliedIndex:87394; }","duration":"109.490427ms","start":"2022-11-07T13:58:24.474Z","end":"2022-11-07T13:58:24.584Z","steps":["trace[1712520298] 'read index received'  (duration: 109.478124ms)","trace[1712520298] 'applied index is now lower than readState.Index'  (duration: 10.049¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:58:24.607Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"132.281885ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:58:24.607Z","caller":"traceutil/trace.go:171","msg":"trace[2079886206] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:68124; }","duration":"132.421486ms","start":"2022-11-07T13:58:24.474Z","end":"2022-11-07T13:58:24.607Z","steps":["trace[2079886206] 'agreement among raft nodes before linearized reading'  (duration: 109.616873ms)","trace[2079886206] 'range keys from in-memory index tree'  (duration: 22.638242ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:58:28.601Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"126.675761ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:58:28.601Z","caller":"traceutil/trace.go:171","msg":"trace[4361028] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:68128; }","duration":"126.786518ms","start":"2022-11-07T13:58:28.474Z","end":"2022-11-07T13:58:28.601Z","steps":["trace[4361028] 'agreement among raft nodes before linearized reading'  (duration: 89.953373ms)","trace[4361028] 'range keys from in-memory index tree'  (duration: 36.689366ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:58:36.800Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"101.123831ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-07T13:58:36.800Z","caller":"traceutil/trace.go:171","msg":"trace[1397413869] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:68134; }","duration":"101.250708ms","start":"2022-11-07T13:58:36.699Z","end":"2022-11-07T13:58:36.800Z","steps":["trace[1397413869] 'agreement among raft nodes before linearized reading'  (duration: 90.117304ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-07T13:59:03.084Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"200.400854ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016914834979498 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:68145 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:68 lease:8128016914834979496 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-07T13:59:03.084Z","caller":"traceutil/trace.go:171","msg":"trace[1485660702] transaction","detail":"{read_only:false; response_revision:68151; number_of_response:1; }","duration":"286.499869ms","start":"2022-11-07T13:59:02.798Z","end":"2022-11-07T13:59:03.084Z","steps":["trace[1485660702] 'process raft request'  (duration: 85.754621ms)","trace[1485660702] 'compare'  (duration: 200.274016ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:59:03.084Z","caller":"traceutil/trace.go:171","msg":"trace[426060304] transaction","detail":"{read_only:false; response_revision:68152; number_of_response:1; }","duration":"117.158589ms","start":"2022-11-07T13:59:02.967Z","end":"2022-11-07T13:59:03.084Z","steps":["trace[426060304] 'process raft request'  (duration: 116.963874ms)"],"step_count":1}
{"level":"info","ts":"2022-11-07T13:59:03.196Z","caller":"traceutil/trace.go:171","msg":"trace[64149233] linearizableReadLoop","detail":"{readStateIndex:87430; appliedIndex:87430; }","duration":"108.349285ms","start":"2022-11-07T13:59:03.088Z","end":"2022-11-07T13:59:03.196Z","steps":["trace[64149233] 'read index received'  (duration: 108.340168ms)","trace[64149233] 'applied index is now lower than readState.Index'  (duration: 7.564¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-07T13:59:03.285Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"196.814243ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:425"}
{"level":"info","ts":"2022-11-07T13:59:03.285Z","caller":"traceutil/trace.go:171","msg":"trace[1982114196] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:68152; }","duration":"196.934508ms","start":"2022-11-07T13:59:03.088Z","end":"2022-11-07T13:59:03.285Z","steps":["trace[1982114196] 'agreement among raft nodes before linearized reading'  (duration: 108.559457ms)","trace[1982114196] 'range keys from in-memory index tree'  (duration: 88.119211ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:59:03.285Z","caller":"traceutil/trace.go:171","msg":"trace[1130925937] transaction","detail":"{read_only:false; response_revision:68153; number_of_response:1; }","duration":"192.126285ms","start":"2022-11-07T13:59:03.093Z","end":"2022-11-07T13:59:03.285Z","steps":["trace[1130925937] 'process raft request'  (duration: 103.985593ms)","trace[1130925937] 'compare'  (duration: 88.017541ms)"],"step_count":2}
{"level":"info","ts":"2022-11-07T13:59:13.565Z","caller":"traceutil/trace.go:171","msg":"trace[1224109766] transaction","detail":"{read_only:false; response_revision:68160; number_of_response:1; }","duration":"170.881546ms","start":"2022-11-07T13:59:13.394Z","end":"2022-11-07T13:59:13.565Z","steps":["trace[1224109766] 'process raft request'  (duration: 148.304198ms)","trace[1224109766] 'compare'  (duration: 22.383375ms)"],"step_count":2}

* 
* ==> kernel <==
*  13:59:15 up  2:38,  0 users,  load average: 2.16, 2.50, 3.85
Linux minikube 5.15.0-52-generic #58~20.04.1-Ubuntu SMP Thu Oct 13 13:09:46 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [49b4ef3c7b74] <==
* Trace[142891837]: [568.344139ms] [568.344139ms] END
I1107 13:47:54.136937       1 trace.go:205] Trace[1571072945]: "GuaranteedUpdate etcd3" type:*core.Endpoints (07-Nov-2022 13:47:53.592) (total time: 544ms):
Trace[1571072945]: ---"Transaction committed" 543ms (13:47:54.136)
Trace[1571072945]: [544.422141ms] [544.422141ms] END
I1107 13:47:54.137207       1 trace.go:205] Trace[651701451]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:7c56cb77-1a1b-4762-858b-d31a5fb2ef8d,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (07-Nov-2022 13:47:53.591) (total time: 545ms):
Trace[651701451]: ---"Object stored in database" 544ms (13:47:54.136)
Trace[651701451]: [545.161635ms] [545.161635ms] END
I1107 13:47:59.368202       1 trace.go:205] Trace[1626702233]: "GuaranteedUpdate etcd3" type:*core.Endpoints (07-Nov-2022 13:47:58.417) (total time: 950ms):
Trace[1626702233]: ---"Transaction committed" 950ms (13:47:59.367)
Trace[1626702233]: [950.668756ms] [950.668756ms] END
I1107 13:47:59.368438       1 trace.go:205] Trace[792507219]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:e4352d96-12eb-4774-b590-4b081627361e,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (07-Nov-2022 13:47:58.417) (total time: 951ms):
Trace[792507219]: ---"Object stored in database" 950ms (13:47:59.368)
Trace[792507219]: [951.238918ms] [951.238918ms] END
I1107 13:48:04.037879       1 trace.go:205] Trace[1194347176]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:98414009-37d9-4162-ab01-498caf30e352,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (07-Nov-2022 13:48:03.407) (total time: 630ms):
Trace[1194347176]: ---"About to write a response" 630ms (13:48:04.037)
Trace[1194347176]: [630.333833ms] [630.333833ms] END
I1107 13:48:04.868258       1 trace.go:205] Trace[1990548695]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (07-Nov-2022 13:48:03.331) (total time: 1536ms):
Trace[1990548695]: ---"initial value restored" 706ms (13:48:04.037)
Trace[1990548695]: ---"Transaction prepared" 292ms (13:48:04.330)
Trace[1990548695]: ---"Transaction committed" 538ms (13:48:04.868)
Trace[1990548695]: [1.536465102s] [1.536465102s] END
I1107 13:48:04.868711       1 trace.go:205] Trace[437483999]: "GuaranteedUpdate etcd3" type:*core.Endpoints (07-Nov-2022 13:48:04.047) (total time: 821ms):
Trace[437483999]: ---"Transaction committed" 763ms (13:48:04.868)
Trace[437483999]: [821.230848ms] [821.230848ms] END
I1107 13:48:04.868943       1 trace.go:205] Trace[205263845]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:06d5e5ae-ee65-45e7-98ed-72d158bfb3d7,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (07-Nov-2022 13:48:04.047) (total time: 821ms):
Trace[205263845]: ---"Object stored in database" 821ms (13:48:04.868)
Trace[205263845]: [821.818323ms] [821.818323ms] END
I1107 13:48:13.271306       1 trace.go:205] Trace[1132725534]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (07-Nov-2022 13:48:12.756) (total time: 514ms):
Trace[1132725534]: ---"Transaction committed" 512ms (13:48:13.271)
Trace[1132725534]: [514.867782ms] [514.867782ms] END
I1107 13:48:53.323727       1 trace.go:205] Trace[1502636299]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (07-Nov-2022 13:48:52.758) (total time: 564ms):
Trace[1502636299]: ---"Transaction committed" 562ms (13:48:53.323)
Trace[1502636299]: [564.996501ms] [564.996501ms] END
I1107 13:49:03.349980       1 trace.go:205] Trace[360581227]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:dfbd5063-215c-49cd-963b-1ee73716375e,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (07-Nov-2022 13:49:02.753) (total time: 595ms):
Trace[360581227]: ---"About to write a response" 595ms (13:49:03.349)
Trace[360581227]: [595.889098ms] [595.889098ms] END
I1107 13:49:03.894077       1 trace.go:205] Trace[1756888488]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (07-Nov-2022 13:49:03.354) (total time: 539ms):
Trace[1756888488]: ---"Transaction committed" 538ms (13:49:03.894)
Trace[1756888488]: [539.81978ms] [539.81978ms] END
I1107 13:50:51.876065       1 trace.go:205] Trace[1702474535]: "GuaranteedUpdate etcd3" type:*coordination.Lease (07-Nov-2022 13:50:51.287) (total time: 588ms):
Trace[1702474535]: ---"Transaction committed" 587ms (13:50:51.875)
Trace[1702474535]: [588.103439ms] [588.103439ms] END
I1107 13:50:51.876383       1 trace.go:205] Trace[476324249]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:Go-http-client/2.0,audit-id:8693c8c0-4341-4831-aef7-562f3d2eb243,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (07-Nov-2022 13:50:51.287) (total time: 588ms):
Trace[476324249]: ---"Object stored in database" 588ms (13:50:51.876)
Trace[476324249]: [588.615331ms] [588.615331ms] END
I1107 13:50:51.916279       1 trace.go:205] Trace[343537958]: "GuaranteedUpdate etcd3" type:*core.Endpoints (07-Nov-2022 13:50:51.159) (total time: 756ms):
Trace[343537958]: ---"Transaction committed" 756ms (13:50:51.916)
Trace[343537958]: [756.969149ms] [756.969149ms] END
I1107 13:50:51.916535       1 trace.go:205] Trace[1245152844]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:69a6aef8-f516-4cd7-bb0c-0eb460a05540,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (07-Nov-2022 13:50:51.158) (total time: 757ms):
Trace[1245152844]: ---"Object stored in database" 757ms (13:50:51.916)
Trace[1245152844]: [757.495959ms] [757.495959ms] END
I1107 13:51:25.338635       1 trace.go:205] Trace[402432588]: "GuaranteedUpdate etcd3" type:*core.Node (07-Nov-2022 13:51:24.003) (total time: 1334ms):
Trace[402432588]: ---"Transaction prepared" 1186ms (13:51:25.190)
Trace[402432588]: [1.334986102s] [1.334986102s] END
I1107 13:51:25.435748       1 trace.go:205] Trace[398574871]: "Patch" url:/api/v1/nodes/minikube/status,user-agent:Go-http-client/2.0,audit-id:01abf29f-064a-4452-a306-5cf563c3751e,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (07-Nov-2022 13:51:23.553) (total time: 1882ms):
Trace[398574871]: ---"Recorded the audit event" 215ms (13:51:23.768)
Trace[398574871]: ---"About to apply patch" 234ms (13:51:24.003)
Trace[398574871]: ---"About to check admission control" 925ms (13:51:24.928)
Trace[398574871]: ---"Object stored in database" 409ms (13:51:25.338)
Trace[398574871]: [1.882110946s] [1.882110946s] END

* 
* ==> kube-apiserver [7cbb8a163e77] <==
* I1103 09:57:42.902914       1 trace.go:205] Trace[1791118043]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:Go-http-client/2.0,audit-id:21d827a6-a857-4d33-8f5a-6d50ee81bf45,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Nov-2022 09:57:41.629) (total time: 1273ms):
Trace[1791118043]: ---"Object stored in database" 1273ms (09:57:42.902)
Trace[1791118043]: [1.273583391s] [1.273583391s] END
I1103 09:57:43.455828       1 trace.go:205] Trace[1747986271]: "Get" url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:eba7ab13-e91a-41aa-ad68-91f594a9f3b7,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Nov-2022 09:57:42.903) (total time: 551ms):
Trace[1747986271]: ---"About to write a response" 551ms (09:57:43.455)
Trace[1747986271]: [551.776681ms] [551.776681ms] END
I1103 09:57:43.456282       1 trace.go:205] Trace[1726131287]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:f0adade0-7125-411d-a883-5f29346ea388,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (03-Nov-2022 09:57:42.678) (total time: 777ms):
Trace[1726131287]: ---"About to write a response" 777ms (09:57:43.456)
Trace[1726131287]: [777.460001ms] [777.460001ms] END
I1103 09:58:50.154064       1 trace.go:205] Trace[519191500]: "GuaranteedUpdate etcd3" type:*core.Endpoints (03-Nov-2022 09:58:48.972) (total time: 1181ms):
Trace[519191500]: ---"Transaction committed" 1180ms (09:58:50.153)
Trace[519191500]: [1.181364415s] [1.181364415s] END
I1103 09:58:50.154437       1 trace.go:205] Trace[1731684632]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:fe904953-b0e0-4d6f-86ca-f34866aec262,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (03-Nov-2022 09:58:48.972) (total time: 1182ms):
Trace[1731684632]: ---"Object stored in database" 1181ms (09:58:50.154)
Trace[1731684632]: [1.182065887s] [1.182065887s] END
I1103 09:58:50.202679       1 trace.go:205] Trace[922688806]: "Get" url:/api/v1/namespaces/kube-system,user-agent:kube-apiserver/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:892599ef-8b7b-475b-89fd-349eee686918,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Nov-2022 09:58:49.632) (total time: 570ms):
Trace[922688806]: ---"About to write a response" 570ms (09:58:50.202)
Trace[922688806]: [570.179077ms] [570.179077ms] END
I1103 11:52:43.040730       1 trace.go:205] Trace[1662451414]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (03-Nov-2022 11:52:42.056) (total time: 520ms):
Trace[1662451414]: ---"Transaction committed" 518ms (11:52:42.577)
Trace[1662451414]: [520.630925ms] [520.630925ms] END
I1103 12:56:36.934492       1 trace.go:205] Trace[801647798]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:2edab241-9e22-4afb-9f94-60d903ad199b,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (03-Nov-2022 12:56:35.024) (total time: 1838ms):
Trace[801647798]: ---"About to write a response" 1838ms (12:56:36.862)
Trace[801647798]: [1.838300205s] [1.838300205s] END
I1103 12:56:36.934521       1 trace.go:205] Trace[1568479970]: "GuaranteedUpdate etcd3" type:*coordination.Lease (03-Nov-2022 12:56:34.296) (total time: 2071ms):
Trace[1568479970]: ---"Transaction committed" 2070ms (12:56:36.367)
Trace[1568479970]: [2.071119533s] [2.071119533s] END
I1103 12:56:36.934833       1 trace.go:205] Trace[155510552]: "GuaranteedUpdate etcd3" type:*core.Event (03-Nov-2022 12:56:33.800) (total time: 2566ms):
Trace[155510552]: ---"Transaction committed" 2562ms (12:56:36.367)
Trace[155510552]: [2.566609445s] [2.566609445s] END
I1103 12:56:36.935276       1 trace.go:205] Trace[935410915]: "Patch" url:/api/v1/namespaces/default/events/sdp-app-7f8b5587cb-d65jt.1723fc6390f0eca7,user-agent:kubelet/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:ba2795d4-4bcf-4a37-be7e-0fd589e2138d,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Nov-2022 12:56:33.800) (total time: 3134ms):
Trace[935410915]: ---"Object stored in database" 3132ms (12:56:36.934)
Trace[935410915]: [3.13465165s] [3.13465165s] END
I1103 12:56:36.934874       1 trace.go:205] Trace[1006426285]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:Go-http-client/2.0,audit-id:ae093388-4f67-4809-83d4-81c0635eca09,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (03-Nov-2022 12:56:34.296) (total time: 2638ms):
Trace[1006426285]: ---"Object stored in database" 2638ms (12:56:36.934)
Trace[1006426285]: [2.638650995s] [2.638650995s] END
{"level":"warn","ts":"2022-11-03T12:56:37.390Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0016b4540/#initially=[https://127.0.0.1:2379]","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I1103 13:21:32.918213       1 trace.go:205] Trace[428712447]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:4e8e1120-c29f-4496-92b0-3027999fbd27,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Nov-2022 13:21:32.363) (total time: 554ms):
Trace[428712447]: ---"About to write a response" 554ms (13:21:32.918)
Trace[428712447]: [554.467154ms] [554.467154ms] END
I1103 13:51:43.679024       1 trace.go:205] Trace[1681991295]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (03-Nov-2022 13:51:42.719) (total time: 959ms):
Trace[1681991295]: ---"Transaction committed" 956ms (13:51:43.678)
Trace[1681991295]: [959.713915ms] [959.713915ms] END
I1103 13:51:44.520134       1 trace.go:205] Trace[2034039059]: "Get" url:/apis/flowcontrol.apiserver.k8s.io/v1beta2/flowschemas/system-nodes,user-agent:kube-apiserver/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:9b67fa96-b3be-4d92-a82c-ae9e9f3c0419,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Nov-2022 13:51:43.818) (total time: 701ms):
Trace[2034039059]: ---"About to write a response" 701ms (13:51:44.519)
Trace[2034039059]: [701.777379ms] [701.777379ms] END
I1103 13:51:45.076641       1 trace.go:205] Trace[179606416]: "Get" url:/apis/flowcontrol.apiserver.k8s.io/v1beta2/flowschemas/system-leader-election,user-agent:kube-apiserver/v1.23.3 (linux/amd64) kubernetes/816c97a,audit-id:1fec9e40-54b6-4fdd-9fbe-2437c781a4a8,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (03-Nov-2022 13:51:44.533) (total time: 543ms):
Trace[179606416]: ---"About to write a response" 543ms (13:51:45.076)
Trace[179606416]: [543.336776ms] [543.336776ms] END
I1103 13:52:20.628378       1 object_count_tracker.go:84] "StorageObjectCountTracker pruner is exiting"
I1103 13:52:21.695847       1 controller.go:186] Shutting down kubernetes service endpoint reconciler
I1103 13:52:21.952874       1 dynamic_cafile_content.go:170] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1103 13:52:21.952893       1 dynamic_cafile_content.go:170] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1103 13:52:21.952982       1 available_controller.go:503] Shutting down AvailableConditionController
I1103 13:52:21.998169       1 apiservice_controller.go:131] Shutting down APIServiceRegistrationController
I1103 13:52:22.022632       1 customresource_discovery_controller.go:245] Shutting down DiscoveryController
I1103 13:52:22.022814       1 dynamic_serving_content.go:145] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1103 13:52:22.022913       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1103 13:52:22.029957       1 autoregister_controller.go:165] Shutting down autoregister controller
I1103 13:52:22.034630       1 controller.go:89] Shutting down OpenAPI AggregationController

* 
* ==> kube-controller-manager [0507240798b3] <==
* I1103 01:19:07.471628       1 node_lifecycle_controller.go:1397] Initializing eviction metric for zone: 
W1103 01:19:07.471951       1 node_lifecycle_controller.go:1012] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1103 01:19:07.472122       1 node_lifecycle_controller.go:1213] Controller detected that zone  is now in state Normal.
I1103 01:19:07.586777       1 shared_informer.go:247] Caches are synced for HPA 
I1103 01:19:07.586928       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I1103 01:19:07.589807       1 shared_informer.go:247] Caches are synced for crt configmap 
I1103 01:19:07.590436       1 shared_informer.go:247] Caches are synced for GC 
I1103 01:19:07.590763       1 shared_informer.go:247] Caches are synced for ReplicationController 
I1103 01:19:07.590914       1 shared_informer.go:247] Caches are synced for daemon sets 
I1103 01:19:07.590958       1 shared_informer.go:247] Caches are synced for ephemeral 
I1103 01:19:07.592246       1 shared_informer.go:247] Caches are synced for persistent volume 
I1103 01:19:07.592478       1 shared_informer.go:247] Caches are synced for stateful set 
I1103 01:19:07.592597       1 shared_informer.go:247] Caches are synced for PVC protection 
I1103 01:19:07.592634       1 shared_informer.go:247] Caches are synced for endpoint 
I1103 01:19:07.601958       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I1103 01:19:07.602020       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I1103 01:19:07.602107       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I1103 01:19:07.602438       1 event.go:294] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1103 01:19:07.676530       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I1103 01:19:07.676592       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I1103 01:19:07.676800       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I1103 01:19:07.676888       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I1103 01:19:07.677027       1 shared_informer.go:247] Caches are synced for attach detach 
I1103 01:19:07.821406       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I1103 01:19:07.823431       1 shared_informer.go:247] Caches are synced for deployment 
I1103 01:19:07.901495       1 shared_informer.go:247] Caches are synced for disruption 
I1103 01:19:07.901518       1 disruption.go:371] Sending events to api server.
I1103 01:19:07.983162       1 shared_informer.go:247] Caches are synced for resource quota 
I1103 01:19:07.983265       1 shared_informer.go:247] Caches are synced for resource quota 
I1103 01:19:07.983286       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I1103 01:19:08.059309       1 trace.go:205] Trace[175175389]: "DeltaFIFO Pop Process" ID:catch-all,Depth:11,Reason:slow event handlers blocking the queue (03-Nov-2022 01:19:07.957) (total time: 101ms):
Trace[175175389]: [101.817173ms] [101.817173ms] END
I1103 01:19:08.127226       1 shared_informer.go:247] Caches are synced for garbage collector 
I1103 01:19:08.180913       1 shared_informer.go:247] Caches are synced for garbage collector 
I1103 01:19:08.127259       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
W1103 01:19:08.370389       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kubernetes-dashboard/kubernetes-dashboard", retrying. Error: EndpointSlice informer cache is out of date
W1103 01:19:08.370735       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kubernetes-dashboard/dashboard-metrics-scraper", retrying. Error: EndpointSlice informer cache is out of date
W1103 01:19:08.370853       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kube-system/kube-dns", retrying. Error: EndpointSlice informer cache is out of date
I1103 05:17:04.481018       1 event.go:294] "Event occurred" object="default/sdp-app-5b87876848" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5b87876848-mx5hq"
I1103 05:17:04.481037       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-5b87876848 to 5"
I1103 05:17:04.481210       1 event.go:294] "Event occurred" object="default/sdp-app-5b87876848" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5b87876848-vk5p5"
I1103 05:17:04.710656       1 event.go:294] "Event occurred" object="default/sdp-app-5b87876848" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5b87876848-9kxlp"
I1103 05:17:04.954102       1 event.go:294] "Event occurred" object="default/sdp-app-5b87876848" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5b87876848-qm79s"
I1103 05:17:04.954156       1 event.go:294] "Event occurred" object="default/sdp-app-5b87876848" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5b87876848-95m7l"
I1103 05:19:32.453236       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-569cd6f548 to 3"
I1103 05:19:32.566085       1 event.go:294] "Event occurred" object="default/sdp-app-569cd6f548" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-569cd6f548-lvj2r"
I1103 05:19:32.634854       1 event.go:294] "Event occurred" object="default/sdp-app-569cd6f548" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-569cd6f548-6njg6"
I1103 05:19:32.635470       1 event.go:294] "Event occurred" object="default/sdp-app-569cd6f548" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-569cd6f548-whpjc"
I1103 05:22:06.101258       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-5856659d95 to 3"
I1103 05:22:06.135153       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-k9zw7"
I1103 05:22:06.159184       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-wlnw2"
I1103 05:22:06.163613       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-m2kjc"
I1103 05:34:39.082599       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-7f8b5587cb to 1"
I1103 05:34:39.116602       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-7pn25"
W1103 05:35:55.147482       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/sdp-app-service", retrying. Error: EndpointSlice informer cache is out of date
I1103 05:35:55.319671       1 event.go:294] "Event occurred" object="default/sdp-app-service" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint default/sdp-app-service: Operation cannot be fulfilled on endpoints \"sdp-app-service\": the object has been modified; please apply your changes to the latest version and try again"
I1103 05:36:29.921601       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-7f8b5587cb to 3"
I1103 05:36:29.956803       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-d65jt"
I1103 05:36:29.969397       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-flc6l"
I1103 05:36:29.970543       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-w96m4"

* 
* ==> kube-controller-manager [5e666a149161] <==
* I1107 12:07:07.121361       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-s24xc"
I1107 12:07:26.210487       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-7f8b5587cb to 3"
I1107 12:07:26.482069       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-sfpfg"
I1107 12:07:26.506270       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-7dpdm"
I1107 12:07:26.506713       1 event.go:294] "Event occurred" object="default/sdp-app-7f8b5587cb" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-7f8b5587cb-mtc5m"
W1107 12:18:50.144404       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "k8s-sdp/sdp-app-k8s-service", retrying. Error: EndpointSlice informer cache is out of date
E1107 12:19:29.133154       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:29.306175       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:29.591673       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:29.806155       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:30.104404       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:30.455350       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:30.780930       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:31.369441       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:32.278516       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:33.724985       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:36.455969       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:41.967272       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:19:52.411717       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:20:13.085022       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:20:54.276913       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:21:12.346298       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:22:12.835672       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:23:13.097706       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:24:13.405302       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:25:13.567391       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:26:12.351931       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:27:12.725072       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:28:13.285416       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:29:13.453679       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:30:13.735109       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:31:12.456483       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:32:12.769881       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:33:13.042462       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:34:14.365678       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:35:15.193729       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:36:13.747401       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:37:14.456871       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:38:15.858980       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
I1107 12:39:10.137636       1 event.go:294] "Event occurred" object="default/dtl-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dtl-app-759cfbc945 to 2"
I1107 12:39:11.305658       1 event.go:294] "Event occurred" object="default/dtl-app-759cfbc945" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dtl-app-759cfbc945-rvg2s"
I1107 12:39:11.306118       1 event.go:294] "Event occurred" object="default/dtl-app-759cfbc945" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dtl-app-759cfbc945-z98kv"
E1107 12:39:17.168139       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
E1107 12:40:18.169244       1 namespace_controller.go:162] deletion of namespace k8s-sdp failed: unexpected items still remain in namespace: k8s-sdp for gvr: /v1, Resource=pods
I1107 12:41:17.551490       1 namespace_controller.go:185] Namespace has been deleted k8s-sdp
I1107 12:57:25.038387       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-5856659d95 to 2"
I1107 12:57:26.040010       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-rp8bl"
I1107 12:57:26.040231       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-hjz6p"
I1107 13:12:34.154987       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-646fd967cc to 2"
I1107 13:12:34.579772       1 event.go:294] "Event occurred" object="default/sdp-app-646fd967cc" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-646fd967cc-v5bcl"
I1107 13:12:34.579922       1 event.go:294] "Event occurred" object="default/sdp-app-646fd967cc" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-646fd967cc-qtnkx"
I1107 13:40:42.818812       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-5856659d95 to 3"
I1107 13:40:43.706854       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-k9lbg"
W1107 13:40:43.911271       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/sdp-app", retrying. Error: EndpointSlice informer cache is out of date
I1107 13:40:43.988347       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-mzc8l"
I1107 13:40:43.995313       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-gjcv5"
I1107 13:44:05.603352       1 event.go:294] "Event occurred" object="default/sdp-app" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set sdp-app-5856659d95 to 3"
I1107 13:44:05.912032       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-vrvh5"
I1107 13:44:06.319239       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-npds4"
I1107 13:44:06.319404       1 event.go:294] "Event occurred" object="default/sdp-app-5856659d95" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: sdp-app-5856659d95-55cs7"

* 
* ==> kube-proxy [e299c4606fa5] <==
* I1107 11:47:09.395557       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1107 11:47:09.395738       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1107 11:47:09.395812       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1107 11:47:13.548105       1 server_others.go:206] "Using iptables Proxier"
I1107 11:47:13.548158       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1107 11:47:13.548186       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1107 11:47:13.548240       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1107 11:47:13.549188       1 server.go:656] "Version info" version="v1.23.3"
I1107 11:47:13.755170       1 config.go:317] "Starting service config controller"
I1107 11:47:13.755197       1 shared_informer.go:240] Waiting for caches to sync for service config
I1107 11:47:13.755463       1 config.go:226] "Starting endpoint slice config controller"
I1107 11:47:13.755474       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I1107 11:47:13.855679       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I1107 11:47:13.855679       1 shared_informer.go:247] Caches are synced for service config 
E1107 11:50:28.905768       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30128: bind: address already in use" port={Description:nodePort for k8s-sdp/sdp-app-k8s-service IP: IPFamily:4 Port:30128 Protocol:TCP}
E1107 12:39:11.277614       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30590: bind: address already in use" port={Description:nodePort for default/dtl-app IP: IPFamily:4 Port:30590 Protocol:TCP}
E1107 12:57:25.737022       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30389: bind: address already in use" port={Description:nodePort for default/sdp-app IP: IPFamily:4 Port:30389 Protocol:TCP}
E1107 13:12:34.034211       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31743: bind: address already in use" port={Description:nodePort for default/sdp-app IP: IPFamily:4 Port:31743 Protocol:TCP}
E1107 13:40:42.924413       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :32265: bind: address already in use" port={Description:nodePort for default/sdp-app IP: IPFamily:4 Port:32265 Protocol:TCP}
E1107 13:44:04.182448       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :31574: bind: address already in use" port={Description:nodePort for default/sdp-app IP: IPFamily:4 Port:31574 Protocol:TCP}
I1107 13:46:24.890149       1 trace.go:205] Trace[1544065884]: "iptables ChainExists" (07-Nov-2022 13:46:13.656) (total time: 9231ms):
Trace[1544065884]: [9.23128266s] [9.23128266s] END
I1107 13:46:24.890149       1 trace.go:205] Trace[1869225049]: "iptables ChainExists" (07-Nov-2022 13:46:13.554) (total time: 9333ms):
Trace[1869225049]: [9.333679404s] [9.333679404s] END
I1107 13:47:17.350231       1 trace.go:205] Trace[1023205561]: "iptables ChainExists" (07-Nov-2022 13:47:13.909) (total time: 3440ms):
Trace[1023205561]: [3.440197155s] [3.440197155s] END
I1107 13:47:17.350269       1 trace.go:205] Trace[1868826713]: "iptables ChainExists" (07-Nov-2022 13:47:13.554) (total time: 3673ms):
Trace[1868826713]: [3.673591794s] [3.673591794s] END

* 
* ==> kube-scheduler [1ed89d5b1d77] <==
* W1107 11:45:38.133675       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:38.133746       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:38.345464       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:38.345554       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:38.482418       1 reflector.go:324] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:38.482552       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:38.515271       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:38.515374       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:38.606343       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:38.606439       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:38.654153       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:38.654282       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:38.851938       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:38.852028       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:42.972871       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:42.972910       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:44.143950       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1107 11:45:44.144007       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1107 11:45:50.029096       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1107 11:45:50.029135       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1107 11:45:50.029194       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1107 11:45:50.029146       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1107 11:45:50.029102       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1107 11:45:50.029549       1 reflector.go:324] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1107 11:45:50.029661       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1107 11:45:50.029598       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1107 11:45:50.029091       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1107 11:45:50.030234       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1107 11:45:50.030252       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1107 11:45:50.030428       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1107 11:45:50.029899       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1107 11:45:50.030469       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1107 11:45:50.030010       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1107 11:45:50.030523       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1107 11:45:50.030010       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1107 11:45:50.030580       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1107 11:45:50.030113       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1107 11:45:50.030615       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1107 11:45:50.029881       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1107 11:45:50.030707       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1107 11:45:50.030339       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1107 11:45:50.030748       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1107 11:45:50.171545       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1107 11:45:50.171594       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
I1107 11:46:12.626359       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I1107 12:39:14.174091       1 trace.go:205] Trace[68566661]: "Scheduling" namespace:default,name:dtl-app-759cfbc945-rvg2s (07-Nov-2022 12:39:12.833) (total time: 944ms):
Trace[68566661]: ---"Snapshotting scheduler cache and node infos done" 121ms (12:39:12.955)
Trace[68566661]: ---"Computing predicates done" 822ms (12:39:13.778)
Trace[68566661]: [944.835357ms] [944.835357ms] END
I1107 12:57:27.208168       1 trace.go:205] Trace[2005176120]: "Scheduling" namespace:default,name:sdp-app-5856659d95-rp8bl (07-Nov-2022 12:57:26.346) (total time: 708ms):
Trace[2005176120]: ---"Snapshotting scheduler cache and node infos done" 242ms (12:57:26.589)
Trace[2005176120]: ---"Computing predicates done" 465ms (12:57:27.055)
Trace[2005176120]: [708.189044ms] [708.189044ms] END
I1107 13:12:35.128161       1 trace.go:205] Trace[128665127]: "Scheduling" namespace:default,name:sdp-app-646fd967cc-v5bcl (07-Nov-2022 13:12:34.674) (total time: 244ms):
Trace[128665127]: ---"Computing predicates done" 244ms (13:12:34.919)
Trace[128665127]: [244.594019ms] [244.594019ms] END
I1107 13:44:08.250868       1 trace.go:205] Trace[1406978664]: "Scheduling" namespace:default,name:sdp-app-5856659d95-vrvh5 (07-Nov-2022 13:44:07.056) (total time: 794ms):
Trace[1406978664]: ---"Snapshotting scheduler cache and node infos done" 253ms (13:44:07.310)
Trace[1406978664]: ---"Computing predicates done" 540ms (13:44:07.850)
Trace[1406978664]: [794.052459ms] [794.052459ms] END

* 
* ==> kube-scheduler [723efb2ab753] <==
* E1103 01:18:37.134078       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:37.481923       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:37.481979       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:37.530080       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:37.530159       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:37.560212       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:37.560291       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:37.770854       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1beta1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:37.770941       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1beta1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:37.813969       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:37.814029       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:37.923193       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:37.923352       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:37.971962       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:37.972071       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:38.008077       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1103 01:18:38.008166       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1103 01:18:43.246797       1 reflector.go:324] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1103 01:18:43.246831       1 reflector.go:138] k8s.io/apiserver/pkg/server/dynamiccertificates/configmap_cafile_content.go:205: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1103 01:18:43.333299       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1103 01:18:43.333443       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1103 01:18:43.333457       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1103 01:18:43.333484       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1103 01:18:43.333497       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1103 01:18:43.333458       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1103 01:18:43.333649       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1103 01:18:43.333681       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1103 01:18:43.333691       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1103 01:18:43.333301       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1103 01:18:43.333730       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1beta1.CSIStorageCapacity: failed to list *v1beta1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1103 01:18:43.333681       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1103 01:18:43.333767       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1103 01:18:43.333412       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1103 01:18:43.333817       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1103 01:18:43.333711       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1103 01:18:43.333916       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1103 01:18:43.333961       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1103 01:18:43.334155       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1103 01:18:43.334184       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1103 01:18:43.334226       1 reflector.go:324] k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1103 01:18:43.334261       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
I1103 01:18:52.008837       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
E1103 01:18:54.015087       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E1103 01:18:54.015132       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E1103 01:18:54.015168       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kubernetes-dashboard\" not found" namespace="kubernetes-dashboard"
E1103 01:18:54.015198       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kubernetes-dashboard\" not found" namespace="kubernetes-dashboard"
E1103 01:18:54.015217       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E1103 01:18:54.015235       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E1103 01:18:54.015246       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E1103 01:18:54.015284       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
E1103 01:18:54.015315       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
I1103 02:19:41.513946       1 trace.go:205] Trace[1137966944]: "Scheduling" namespace:default,name:myapp-pod (03-Nov-2022 02:19:41.167) (total time: 342ms):
Trace[1137966944]: ---"Computing predicates done" 342ms (02:19:41.510)
Trace[1137966944]: [342.640883ms] [342.640883ms] END
I1103 05:17:04.959202       1 trace.go:205] Trace[1783400979]: "Scheduling" namespace:default,name:sdp-app-5b87876848-mx5hq (03-Nov-2022 05:17:04.701) (total time: 247ms):
Trace[1783400979]: ---"Computing predicates done" 247ms (05:17:04.949)
Trace[1783400979]: [247.922472ms] [247.922472ms] END
I1103 10:00:28.078035       1 trace.go:205] Trace[1879953227]: "Scheduling" namespace:default,name:sdp-app-7f8b5587cb-flc6l (03-Nov-2022 10:00:26.841) (total time: 154ms):
Trace[1879953227]: ---"Computing predicates done" 154ms (10:00:26.995)
Trace[1879953227]: [154.610872ms] [154.610872ms] END

* 
* ==> kubelet <==
* -- Logs begin at Mon 2022-11-07 11:44:04 UTC, end at Mon 2022-11-07 13:59:17 UTC. --
Nov 07 13:40:53 minikube kubelet[972]: I1107 13:40:53.774758     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-k9lbg through plugin: invalid network status for"
Nov 07 13:40:53 minikube kubelet[972]: I1107 13:40:53.783920     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-gjcv5 through plugin: invalid network status for"
Nov 07 13:40:53 minikube kubelet[972]: I1107 13:40:53.791755     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-mzc8l through plugin: invalid network status for"
Nov 07 13:44:09 minikube kubelet[972]: I1107 13:44:09.262447     972 topology_manager.go:200] "Topology Admit Handler"
Nov 07 13:44:10 minikube kubelet[972]: I1107 13:44:10.618859     972 topology_manager.go:200] "Topology Admit Handler"
Nov 07 13:44:10 minikube kubelet[972]: I1107 13:44:10.641544     972 topology_manager.go:200] "Topology Admit Handler"
Nov 07 13:44:10 minikube kubelet[972]: I1107 13:44:10.897394     972 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pxg5x\" (UniqueName: \"kubernetes.io/projected/c53c581c-8f35-4af1-ad46-6d0b85d274c0-kube-api-access-pxg5x\") pod \"sdp-app-5856659d95-npds4\" (UID: \"c53c581c-8f35-4af1-ad46-6d0b85d274c0\") " pod="default/sdp-app-5856659d95-npds4"
Nov 07 13:44:10 minikube kubelet[972]: I1107 13:44:10.897520     972 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4ccmd\" (UniqueName: \"kubernetes.io/projected/5cbc3c8c-af7f-4c02-948a-021e45f042b1-kube-api-access-4ccmd\") pod \"sdp-app-5856659d95-vrvh5\" (UID: \"5cbc3c8c-af7f-4c02-948a-021e45f042b1\") " pod="default/sdp-app-5856659d95-vrvh5"
Nov 07 13:44:10 minikube kubelet[972]: I1107 13:44:10.998073     972 reconciler.go:221] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-d6z95\" (UniqueName: \"kubernetes.io/projected/c8dfa10e-b290-4d0a-a8b5-424647c08c5e-kube-api-access-d6z95\") pod \"sdp-app-5856659d95-55cs7\" (UID: \"c8dfa10e-b290-4d0a-a8b5-424647c08c5e\") " pod="default/sdp-app-5856659d95-55cs7"
Nov 07 13:44:32 minikube kubelet[972]: I1107 13:44:32.783483     972 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="ca060d0d0f4810b85b50537f8600005f31234571d8e0361ea842756b02eb9d29"
Nov 07 13:44:32 minikube kubelet[972]: I1107 13:44:32.783587     972 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="8e504395289e2185732d294031953b4068d326b590d4c6973fa0813349ed6475"
Nov 07 13:44:32 minikube kubelet[972]: I1107 13:44:32.783612     972 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="2d8be2a3b570dc1f10d01dfd00db797aeb8a09445ee7d74cf8d681e928e2023f"
Nov 07 13:44:35 minikube kubelet[972]: I1107 13:44:35.076114     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-gjcv5 through plugin: invalid network status for"
Nov 07 13:44:40 minikube kubelet[972]: I1107 13:44:40.994586     972 scope.go:110] "RemoveContainer" containerID="074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8"
Nov 07 13:44:41 minikube kubelet[972]: I1107 13:44:41.032702     972 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-lvzv2\" (UniqueName: \"kubernetes.io/projected/3db434f6-6d33-420d-a95c-3cfbac0eef50-kube-api-access-lvzv2\") pod \"3db434f6-6d33-420d-a95c-3cfbac0eef50\" (UID: \"3db434f6-6d33-420d-a95c-3cfbac0eef50\") "
Nov 07 13:44:41 minikube kubelet[972]: I1107 13:44:41.133863     972 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-jhgcg\" (UniqueName: \"kubernetes.io/projected/783d8140-5f19-4da2-9c64-5b8ac51e4544-kube-api-access-jhgcg\") pod \"783d8140-5f19-4da2-9c64-5b8ac51e4544\" (UID: \"783d8140-5f19-4da2-9c64-5b8ac51e4544\") "
Nov 07 13:44:42 minikube kubelet[972]: I1107 13:44:42.543063     972 reconciler.go:192] "operationExecutor.UnmountVolume started for volume \"kube-api-access-q6j86\" (UniqueName: \"kubernetes.io/projected/3ae09f70-98ff-475b-832f-2550e6360c09-kube-api-access-q6j86\") pod \"3ae09f70-98ff-475b-832f-2550e6360c09\" (UID: \"3ae09f70-98ff-475b-832f-2550e6360c09\") "
Nov 07 13:44:43 minikube kubelet[972]: I1107 13:44:43.024725     972 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/783d8140-5f19-4da2-9c64-5b8ac51e4544-kube-api-access-jhgcg" (OuterVolumeSpecName: "kube-api-access-jhgcg") pod "783d8140-5f19-4da2-9c64-5b8ac51e4544" (UID: "783d8140-5f19-4da2-9c64-5b8ac51e4544"). InnerVolumeSpecName "kube-api-access-jhgcg". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 07 13:44:43 minikube kubelet[972]: I1107 13:44:43.015008     972 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3db434f6-6d33-420d-a95c-3cfbac0eef50-kube-api-access-lvzv2" (OuterVolumeSpecName: "kube-api-access-lvzv2") pod "3db434f6-6d33-420d-a95c-3cfbac0eef50" (UID: "3db434f6-6d33-420d-a95c-3cfbac0eef50"). InnerVolumeSpecName "kube-api-access-lvzv2". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 07 13:44:43 minikube kubelet[972]: I1107 13:44:43.025909     972 operation_generator.go:910] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3ae09f70-98ff-475b-832f-2550e6360c09-kube-api-access-q6j86" (OuterVolumeSpecName: "kube-api-access-q6j86") pod "3ae09f70-98ff-475b-832f-2550e6360c09" (UID: "3ae09f70-98ff-475b-832f-2550e6360c09"). InnerVolumeSpecName "kube-api-access-q6j86". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 07 13:44:43 minikube kubelet[972]: I1107 13:44:43.228697     972 reconciler.go:300] "Volume detached for volume \"kube-api-access-jhgcg\" (UniqueName: \"kubernetes.io/projected/783d8140-5f19-4da2-9c64-5b8ac51e4544-kube-api-access-jhgcg\") on node \"minikube\" DevicePath \"\""
Nov 07 13:44:43 minikube kubelet[972]: I1107 13:44:43.228767     972 reconciler.go:300] "Volume detached for volume \"kube-api-access-lvzv2\" (UniqueName: \"kubernetes.io/projected/3db434f6-6d33-420d-a95c-3cfbac0eef50-kube-api-access-lvzv2\") on node \"minikube\" DevicePath \"\""
Nov 07 13:44:43 minikube kubelet[972]: I1107 13:44:43.228804     972 reconciler.go:300] "Volume detached for volume \"kube-api-access-q6j86\" (UniqueName: \"kubernetes.io/projected/3ae09f70-98ff-475b-832f-2550e6360c09-kube-api-access-q6j86\") on node \"minikube\" DevicePath \"\""
Nov 07 13:44:46 minikube kubelet[972]: I1107 13:44:46.478190     972 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=3ae09f70-98ff-475b-832f-2550e6360c09 path="/var/lib/kubelet/pods/3ae09f70-98ff-475b-832f-2550e6360c09/volumes"
Nov 07 13:44:47 minikube kubelet[972]: I1107 13:44:47.010073     972 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=3db434f6-6d33-420d-a95c-3cfbac0eef50 path="/var/lib/kubelet/pods/3db434f6-6d33-420d-a95c-3cfbac0eef50/volumes"
Nov 07 13:44:47 minikube kubelet[972]: I1107 13:44:47.233768     972 scope.go:110] "RemoveContainer" containerID="b50ab5333594b6f6398f3ae89cb135cf749d363d029fc34f3060e3c39a1f7a97"
Nov 07 13:44:48 minikube kubelet[972]: I1107 13:44:48.340349     972 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=783d8140-5f19-4da2-9c64-5b8ac51e4544 path="/var/lib/kubelet/pods/783d8140-5f19-4da2-9c64-5b8ac51e4544/volumes"
Nov 07 13:44:53 minikube kubelet[972]: I1107 13:44:53.989185     972 scope.go:110] "RemoveContainer" containerID="40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686"
Nov 07 13:44:54 minikube kubelet[972]: I1107 13:44:54.938958     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-55cs7 through plugin: invalid network status for"
Nov 07 13:44:54 minikube kubelet[972]: I1107 13:44:54.942000     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-vrvh5 through plugin: invalid network status for"
Nov 07 13:44:56 minikube kubelet[972]: I1107 13:44:56.099094     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-npds4 through plugin: invalid network status for"
Nov 07 13:44:56 minikube kubelet[972]: I1107 13:44:56.107697     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-npds4 through plugin: invalid network status for"
Nov 07 13:44:56 minikube kubelet[972]: I1107 13:44:56.141429     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-vrvh5 through plugin: invalid network status for"
Nov 07 13:44:56 minikube kubelet[972]: I1107 13:44:56.145320     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-55cs7 through plugin: invalid network status for"
Nov 07 13:44:56 minikube kubelet[972]: E1107 13:44:56.160267     972 cadvisor_stats_provider.go:414] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods/podc53c581c-8f35-4af1-ad46-6d0b85d274c0/ca060d0d0f4810b85b50537f8600005f31234571d8e0361ea842756b02eb9d29\": RecentStats: unable to find data in memory cache]"
Nov 07 13:44:58 minikube kubelet[972]: I1107 13:44:58.265258     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-npds4 through plugin: invalid network status for"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.392027     972 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 121dca960321a7ffbca0b1b972d01eccb70648f92d535ce61b9f8c322e6030c3" containerID="121dca960321a7ffbca0b1b972d01eccb70648f92d535ce61b9f8c322e6030c3"
Nov 07 13:44:58 minikube kubelet[972]: I1107 13:44:58.422748     972 scope.go:110] "RemoveContainer" containerID="074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.423742     972 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8" containerID="074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.477139     972 kuberuntime_manager.go:1072] "getPodContainerStatuses for pod failed" err="rpc error: code = Unknown desc = Error: No such container: 121dca960321a7ffbca0b1b972d01eccb70648f92d535ce61b9f8c322e6030c3" pod="default/sdp-app-5856659d95-npds4"
Nov 07 13:44:58 minikube kubelet[972]: I1107 13:44:58.479706     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-vrvh5 through plugin: invalid network status for"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.481522     972 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: fc2c87ebca19756f506d7759f42019e0e024062c82c98c5beb9a06cd8e7d0b1f" containerID="fc2c87ebca19756f506d7759f42019e0e024062c82c98c5beb9a06cd8e7d0b1f"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.481579     972 kuberuntime_manager.go:1072] "getPodContainerStatuses for pod failed" err="rpc error: code = Unknown desc = Error: No such container: fc2c87ebca19756f506d7759f42019e0e024062c82c98c5beb9a06cd8e7d0b1f" pod="default/sdp-app-5856659d95-vrvh5"
Nov 07 13:44:58 minikube kubelet[972]: I1107 13:44:58.483609     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-55cs7 through plugin: invalid network status for"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.485455     972 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 08a556aed964699b81cb8f1c4c65395326d79ba735d17ab51cda6e2283160250" containerID="08a556aed964699b81cb8f1c4c65395326d79ba735d17ab51cda6e2283160250"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.485510     972 kuberuntime_manager.go:1072] "getPodContainerStatuses for pod failed" err="rpc error: code = Unknown desc = Error: No such container: 08a556aed964699b81cb8f1c4c65395326d79ba735d17ab51cda6e2283160250" pod="default/sdp-app-5856659d95-55cs7"
Nov 07 13:44:58 minikube kubelet[972]: I1107 13:44:58.499091     972 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8} err="failed to get container status \"074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8\": rpc error: code = Unknown desc = Error: No such container: 074b0730cb3cd1174f1e57ada3942f329b70480548d993c5bac4fc571d3d3cd8"
Nov 07 13:44:58 minikube kubelet[972]: I1107 13:44:58.499136     972 scope.go:110] "RemoveContainer" containerID="40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686"
Nov 07 13:44:58 minikube kubelet[972]: E1107 13:44:58.500211     972 remote_runtime.go:572] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: 40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686" containerID="40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686"
Nov 07 13:44:58 minikube kubelet[972]: I1107 13:44:58.500267     972 pod_container_deletor.go:52] "DeleteContainer returned error" containerID={Type:docker ID:40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686} err="failed to get container status \"40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686\": rpc error: code = Unknown desc = Error: No such container: 40716e32a4e355223afe3308f218b59cf5fe0cbec137e3df3d01148cd8fbc686"
Nov 07 13:44:59 minikube kubelet[972]: I1107 13:44:59.499570     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-npds4 through plugin: invalid network status for"
Nov 07 13:45:00 minikube kubelet[972]: I1107 13:45:00.783770     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-vrvh5 through plugin: invalid network status for"
Nov 07 13:45:00 minikube kubelet[972]: I1107 13:45:00.981773     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-55cs7 through plugin: invalid network status for"
Nov 07 13:45:02 minikube kubelet[972]: I1107 13:45:02.196629     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-vrvh5 through plugin: invalid network status for"
Nov 07 13:45:02 minikube kubelet[972]: I1107 13:45:02.206727     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-55cs7 through plugin: invalid network status for"
Nov 07 13:45:02 minikube kubelet[972]: I1107 13:45:02.219180     972 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/sdp-app-5856659d95-npds4 through plugin: invalid network status for"
Nov 07 13:46:55 minikube kubelet[972]: I1107 13:46:55.965890     972 trace.go:205] Trace[1187267945]: "iptables ChainExists" (07-Nov-2022 13:46:52.494) (total time: 2430ms):
Nov 07 13:46:55 minikube kubelet[972]: Trace[1187267945]: [2.430999242s] [2.430999242s] END
Nov 07 13:46:55 minikube kubelet[972]: I1107 13:46:55.965892     972 trace.go:205] Trace[1192260296]: "iptables ChainExists" (07-Nov-2022 13:46:52.336) (total time: 2563ms):
Nov 07 13:46:55 minikube kubelet[972]: Trace[1192260296]: [2.563718007s] [2.563718007s] END

* 
* ==> kubernetes-dashboard [66987a5f16e5] <==
* 2022/11/07 11:46:58 Using namespace: kubernetes-dashboard
2022/11/07 11:46:58 Using in-cluster config to connect to apiserver
2022/11/07 11:46:58 Using secret token for csrf signing
2022/11/07 11:46:58 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/11/07 11:46:58 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc0004335a0)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x413
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc0004a7480)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:502 +0xc6
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0xc0004a7480)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:470 +0x47

* 
* ==> kubernetes-dashboard [d29730e8928f] <==
* 2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/11/07 13:16:26 Getting list of namespaces
2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Incoming HTTP/1.1 GET /api/v1/service/default/sdp-app/pod?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Incoming HTTP/1.1 GET /api/v1/service/default/sdp-app/event?itemsPerPage=10&page=1 request from 127.0.0.1: 
2022/11/07 13:16:26 Found 2 events related to sdp-app service in default namespace
2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:26 received 0 resources from sidecar instead of 2
2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Incoming HTTP/1.1 GET /api/v1/service/default/sdp-app request from 127.0.0.1: 
2022/11/07 13:16:26 Getting details of sdp-app service in default namespace
2022/11/07 13:16:26 received 0 resources from sidecar instead of 2
2022/11/07 13:16:26 Getting pod metrics
2022/11/07 13:16:26 received 0 resources from sidecar instead of 2
2022/11/07 13:16:26 Found 1 endpoints related to sdp-app service in default namespace
2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:26 received 0 resources from sidecar instead of 2
2022/11/07 13:16:26 Skipping metric because of error: Metric label not set.
2022/11/07 13:16:26 Skipping metric because of error: Metric label not set.
2022/11/07 13:16:26 Skipping metric because of error: Metric label not set.
2022/11/07 13:16:26 Skipping metric because of error: Metric label not set.
2022/11/07 13:16:26 [2022-11-07T13:16:26Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:30 [2022-11-07T13:16:30Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2022/11/07 13:16:30 [2022-11-07T13:16:30Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:30 [2022-11-07T13:16:30Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/11/07 13:16:30 Getting list of all services in the cluster
2022/11/07 13:16:30 [2022-11-07T13:16:30Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:31 [2022-11-07T13:16:31Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/11/07 13:16:31 Getting list of namespaces
2022/11/07 13:16:31 [2022-11-07T13:16:31Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:35 [2022-11-07T13:16:35Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/11/07 13:16:35 Getting list of all services in the cluster
2022/11/07 13:16:35 [2022-11-07T13:16:35Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:36 [2022-11-07T13:16:36Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/11/07 13:16:36 Getting list of namespaces
2022/11/07 13:16:36 [2022-11-07T13:16:36Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:40 [2022-11-07T13:16:40Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/11/07 13:16:40 Getting list of all services in the cluster
2022/11/07 13:16:40 [2022-11-07T13:16:40Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:41 [2022-11-07T13:16:41Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/11/07 13:16:41 Getting list of namespaces
2022/11/07 13:16:41 [2022-11-07T13:16:41Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:44 [2022-11-07T13:16:44Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/11/07 13:16:44 Getting list of namespaces
2022/11/07 13:16:44 [2022-11-07T13:16:44Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:16:44 [2022-11-07T13:16:44Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/11/07 13:16:44 Getting list of all services in the cluster
2022/11/07 13:16:44 [2022-11-07T13:16:44Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:17:25 [2022-11-07T13:17:25Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/11/07 13:17:25 Getting list of all services in the cluster
2022/11/07 13:17:25 [2022-11-07T13:17:25Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/11/07 13:17:25 Getting list of namespaces
2022/11/07 13:17:25 [2022-11-07T13:17:25Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:17:25 [2022-11-07T13:17:25Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:17:29 [2022-11-07T13:17:29Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/11/07 13:17:29 Getting list of namespaces
2022/11/07 13:17:29 [2022-11-07T13:17:29Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/11/07 13:17:29 Getting list of all services in the cluster
2022/11/07 13:17:29 [2022-11-07T13:17:29Z] Outcoming response to 127.0.0.1 with 200 status code
2022/11/07 13:17:29 [2022-11-07T13:17:29Z] Outcoming response to 127.0.0.1 with 200 status code
I1107 13:47:44.950216       1 request.go:668] Waited for 1.219664385s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-key-holder

* 
* ==> storage-provisioner [565aa3f294e0] <==
* I1107 11:47:15.089441       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1107 11:47:16.193703       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1107 11:47:16.193786       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1107 11:47:33.808281       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1107 11:47:33.808715       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_eb8beee8-6fbc-4c78-940d-a7aa6627c99f!
I1107 11:47:33.809131       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a6916f67-2485-4467-a40d-423e882be75e", APIVersion:"v1", ResourceVersion:"61895", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_eb8beee8-6fbc-4c78-940d-a7aa6627c99f became leader
I1107 11:47:34.065077       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_eb8beee8-6fbc-4c78-940d-a7aa6627c99f!

* 
* ==> storage-provisioner [7eba8c15d7c6] <==
* I1107 11:46:41.788005       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1107 11:46:50.631609       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

